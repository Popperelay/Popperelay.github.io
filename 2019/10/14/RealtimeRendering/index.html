<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
	<link rel="bookmark" type="image/x-icon" href="/img/logo_miccall.png">
	 <link rel="shortcut icon" href="/img/logo_miccall.png">
	
			
    <title>
    Elays'Blog
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
	<script>
	(function(){
		if('{{ page.password }}'){
			if (prompt('请输入博客密码') !== "elayRender"){
				alert('密码错误！');
				window.close();
			}
		}
	})();
	</script>
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    
    	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
			    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<link rel="stylesheet" href="/css/prism-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
    
		
<!-- Layouts -->


<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<!--<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">-->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML">
</script>

<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_undefined.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">MICCALL</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/C/">C#</a></li><li><a class="category-link" href="/categories/C-语法/">C#语法</a></li><li><a class="category-link" href="/categories/C/">C++</a></li><li><a class="category-link" href="/categories/C语言/">C语言</a></li><li><a class="category-link" href="/categories/Graphics/">Graphics</a></li><li><a class="category-link" href="/categories/Math/">Math</a></li><li><a class="category-link" href="/categories/OpenGL/">OpenGL</a></li><li><a class="category-link" href="/categories/Unity/">Unity</a></li><li><a class="category-link" href="/categories/Vulkan/">Vulkan</a></li><li><a class="category-link" href="/categories/博客配置/">博客配置</a></li><li><a class="category-link" href="/categories/并行计算/">并行计算</a></li><li><a class="category-link" href="/categories/操作系统/">操作系统</a></li><li><a class="category-link" href="/categories/数据结构/">数据结构</a></li><li><a class="category-link" href="/categories/毕设/">毕设</a></li><li><a class="category-link" href="/categories/游戏开发/">游戏开发</a></li><li><a class="category-link" href="/categories/计算机系统/">计算机系统</a></li><li><a class="category-link" href="/categories/设计模式/">设计模式</a></li><li><a class="category-link" href="/categories/面试/">面试</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="搜索">
		                搜索
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
		            
		                <li><a href="https://github.com/miccall" class="icon fa-github"><span class="label">GitHub</span></a></li>
		            
		            
		            
		            
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_BG.jpg?raw=true);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>Realtime Rendering</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
				<!-- 目录 -->
				<p class="show-toc-btn" id="show-toc-btn" onclick="showToc();" style="display:none">
				<span class="btn-bg"></span>
				<span class="btn-text">文章导航</span>
				</p>
				<div id="toc-article" class="toc-article">
					<span id="toc-close" class="toc-close" title="隐藏导航" onclick="showBtn();">×</span>
					<strong class="toc-title">文章目录</strong>
					<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-The-Graphics-Rendering-Pipeline"><span class="toc-text">2. The Graphics Rendering Pipeline</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-The-Graphics-Processing-Unit"><span class="toc-text">3. The Graphics Processing Unit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Transform"><span class="toc-text">4. Transform</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#四元数"><span class="toc-text">四元数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#顶点混合"><span class="toc-text">顶点混合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型变形"><span class="toc-text">模型变形</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#投影"><span class="toc-text">投影</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#透视投影"><span class="toc-text">透视投影</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Further-Reading"><span class="toc-text">Further Reading</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Shading-Basics"><span class="toc-text">5. Shading Basics</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Light-Sources"><span class="toc-text">Light Sources</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementing-Shading-Models"><span class="toc-text">Implementing Shading Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Aliasing-and-Antialiasing"><span class="toc-text">Aliasing and Antialiasing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transparency-Alpha-and-Compositing"><span class="toc-text">Transparency, Alpha, and Compositing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Display-Encoding"><span class="toc-text">Display Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Further-Reading-1"><span class="toc-text">Further Reading</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Texturing"><span class="toc-text">6. Texturing</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Texturing-Pipeline"><span class="toc-text">The Texturing Pipeline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Image-Texturing"><span class="toc-text">Image Texturing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Magnification"><span class="toc-text">Magnification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Minification"><span class="toc-text">Minification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Volume-Textures"><span class="toc-text">Volume Textures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Texture-Representation"><span class="toc-text">Texture Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Texture-Compression"><span class="toc-text">Texture Compression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Block-Compression"><span class="toc-text">Block Compression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ETC"><span class="toc-text">ETC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ETC2"><span class="toc-text">ETC2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#压缩法线贴图"><span class="toc-text">压缩法线贴图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PVRTC"><span class="toc-text">PVRTC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ASTC"><span class="toc-text">ASTC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#改善压缩贴图质量"><span class="toc-text">改善压缩贴图质量</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-Non-Photorealistic-Rendering"><span class="toc-text">15. Non-Photorealistic Rendering</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Outline-Rendering"><span class="toc-text">Outline Rendering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#法线和视线夹角"><span class="toc-text">法线和视线夹角</span></a></li></ol></li></ol></li></ol>
			   </div>
			   <script type="text/javascript">
				function showToc(){
					var toc_article = document.getElementById("toc-article");
					var show_toc_btn = document.getElementById("show-toc-btn");
					toc_article.setAttribute("style","display:block");
					show_toc_btn.setAttribute("style","display:none");
					};
				function showBtn(){
					var toc_article = document.getElementById("toc-article");
					var show_toc_btn = document.getElementById("show-toc-btn");
					toc_article.setAttribute("style","display:none");
					show_toc_btn.setAttribute("style","display:block");
					};
			   </script>
				
                <h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a><font size="6" color="orange">1. Introduction</font></h1><hr>
<p>显示设备的刷新频率和显示频率可能是不一样的。</p>
<h1 id="2-The-Graphics-Rendering-Pipeline"><a href="#2-The-Graphics-Rendering-Pipeline" class="headerlink" title="2. The Graphics Rendering Pipeline"></a><font size="6" color="orange">2. The Graphics Rendering Pipeline</font></h1><hr>
<p>投影变换并且透视除法之后，OpenGL的z坐标范围是[-1, 1]，而DirectX的z坐标范围是[0,1]。不过x和y坐标的范围OpenGL和DirectX都是在[-1, 1]的范围。</p>
<h1 id="3-The-Graphics-Processing-Unit"><a href="#3-The-Graphics-Processing-Unit" class="headerlink" title="3. The Graphics Processing Unit"></a><font size="6" color="orange">3. The Graphics Processing Unit</font></h1><hr>
<p>在NVIDIA GPU中，每32个线程(thread)被组成一个线程组(Warp)。比如要并行执行2000个线程，那么它们会被分成2000/32=62.5，也就是63个线程组。第一个线程组先执行，遇到需要取数据时，该线程组会被切换到下一个线程组，由下一个线程组来执行它的程序指令（上一个线程组的数据还是在同步取，只是取数据不需要用到处理器，处理器被切换给下一个线程组了）。这个线程组遇到取数据被阻塞时，依然会切换到下一个线程组。这就是GPU并行执行的流程。如下图所示（图片下面的英文值得看一下~）：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_3_1.png?raw=true" alt><br>上面的线程组交换是GPU隐藏延迟（线程阻塞等待）的主要手段，还有一些其他性能优化技巧，参见参考文献[1]。</p>
<p>影响着色器并行执行的一个重要因素是：着色器需要的寄存器数量。单个线程运行着色器程序所需要的寄存器数量越多，能够同时驻留在GPU上的线程就会越少，对应的线程组也会越少。这会让延迟变的更明显，会有较多的处理器处于空闲状态。线程组驻留在GPU上，称为“in flight”；而驻留在GPU上的线程组的数量称为“驻留率(occupancy)”。低驻留率当然会导致很低的性能。文献[2]概述了着色器使用的寄存器和共享内存的数量，怎样影响到驻留率。文献[3]和文献[4]讨论了着色器执行的不同操作类型，是怎样影响驻留率的。</p>
<p>另一个影响GPU性能的因素是：由if或者循环语句造成的动态分支。如果线程组中的大多数线程都执行if路径，即使只有一个线程会去执行else路径，这整个线程组都会把两条分支路径都执行完，然后每个线程抛掉自己不需要的那个结果。</p>
<p>Shader Model4.0提供的着色器虚拟机架构和寄存器布局如下图所示（英文标题值得一读）：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_3_2.png?raw=true" alt></p>
<p>着色器的分支语句（控制流）分为两种：静态控制流(static flow control)和动态控制流(dynamic flow control)。静态控制流分支是基于Uniform输入的值，它是没有什么线程差异的，因为所有线程都会共用这个Uniform值，也就是说所有线程都是执行的相同的分支路径，所以没有什么太大的性能消耗。但是基于一些在线程之间不相同的值，来分开执行的动态控制流，是会有较大的性能开销的。</p>
<p>大多数情况下，片元着色器的每个线程之间都是相互独立的，无法访问彼此的数据。不过有一个例外：在着色器里使用导数时，实际上可以访问到相邻的线程的信息。GPU会把$2\times 2$个线程合成一个组(group)，然后需要导数信息时，GPU就把对应方向上相邻像素的差值，作为导数值返回给我们。如下图所示（英文题注值得一看）：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_3_3.png?raw=true" alt><br>关于原子计数器或者GPU上的原子单元，可以看一看参考文献[5]。</p>
<p>DirectX里面的UAV（Unordered Access View）类似于OpenGL里的SSBO（Shader Storage Buffer Object），不过DirectX里面还有一个ROVs（Rasterizer Order Views），它可以让片元以一个用户指定的顺序来访问Buffer，而不再是并行无序的访问。这个还是很有用的，比如可以用它来实现自定义的混合算法等等，比管线里的Blend强大得多，比如文献[6]（OpenGL里面有没有这个功能不太清楚~）。</p>
<p>关于eraly-z可以看一看文献[7]、[8]、[9]。</p>
<p>前面说过，我们可以通过ROVs来指定片元的执行顺序，但是无论片元的执行顺序是什么，它们最后都是按照输入的顺序进入到测试阶段，来进行深度、模板等等测试的，跟片元的执行顺序无关。</p>
<p>使用Computer Shader来对图像进行后处理（比如确定一幅图像的平均亮度），会比在Pixel Shader里做要快两倍[9]。</p>
<p>关于更深入地讨论GPU以及图形管线的各个方面，可以看看参考文献[9]。对于GPU的并行性，可以看参考文献[10]。</p>
<p>参考文献：</p>
<ol>
<li>Kubisch, Christoph, “Life of a Triangle—NVIDIA’s Logical Pipeline,” NVIDIA GameWorks blog, Mar. 16, 2015. Cited on p. 32</li>
<li>Lauritzen, Andrew, “Future Directions for Compute-for-Graphics,” SIGGRAPH Open Problems in Real-Time Rendering course, Aug. 2017. Cited on p. 32, 812, 908</li>
<li>Wronski, Bartlomiej, “Assassin’s Creed: Black Flag—Road to Next-Gen Graphics,” Game Developers Conference, Mar. 2014. Cited on p. 32, 218, 478, 571, 572, 801</li>
<li>Wronski, Bartlomiej, “GCN—Two Ways of Latency Hiding and Wave Occupancy,” Bart Wronski blog, Mar. 27, 2014. Cited on p. 32, 801, 1005</li>
<li>Giesen, Fabian, “A Trip through the Graphics Pipeline 2011,” The ryg blog, July 9, 2011. Cited on p. 32, 42, 46, 47, 48, 49, 52, 53, 54, 55, 141, 247, 684, 701, 784, 1040</li>
<li>Bookout, David, “Programmable Blend with Pixel Shader Ordering,” Intel Developer Zone blog, Oct. 13, 2015. Cited on p. 52</li>
<li>Mitchell, Jason L., and Pedro V. Sander, “Applications of Explicit Early-Z Culling,” SIGGRAPH Real-Time Shading course, Aug. 2004. Cited on p. 53, 1016</li>
<li>Sander, Pedro V., Natalya Tatarchuk, and Jason L. Mitchell, “Explicit Early-Z Culling for Efficient Fluid Flow Simulation,” in Wolfgang Engel, ed., ShaderX 5 , Charles River Media, pp. 553–564, 2006. Cited on p. 53, 1016</li>
<li>Giesen, Fabian, “A Trip through the Graphics Pipeline 2011,” The ryg blog, July 9, 2011. Cited on p. 32, 42, 46, 47, 48, 49, 52, 53, 54, 55, 141, 247, 684, 701, 784, 1040</li>
<li>Fatahalian, Kayvon, and Randy Bryant, Parallel Computer Architecture and Programming course, Carnegie Mellon University, Spring 2017. Cited on p. 30, 55</li>
</ol>
<h1 id="4-Transform"><a href="#4-Transform" class="headerlink" title="4. Transform"></a><font size="6" color="orange">4. Transform</font></h1><hr>
<p>线性变换是指满足向量加法和标量乘法的变换，即对于一个变换$f$来说需要满足：<br>$$<br>f(x)+f(y)=f(x+y)\<br>kf(x)=f(kx)<br>$$<br>关于线性代数的本质意义可以参考<a href="[http://popperelay.cn/2019/01/23/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8/#%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%92%8C%E7%9F%A9%E9%98%B5](http://popperelay.cn/2019/01/23/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8/#%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%92%8C%E7%9F%A9%E9%98%B5">这里</a><br>)。</p>
<p>需要注意的是，缩放和旋转都是线性变换，但是平移不是线性变换，因为平移不满足上面说的第一个性质。</p>
<p>仿射矩阵(affine matrix)或者仿射变换(affine transform)的主要特征是：平行线在经过仿射变换后依然会保持平行，但是变换前后线的角度和长度是可能会发生变化的。</p>
<p>对于平移矩阵来说：$T^{-1}(t)=T(-t)$。</p>
<p>无论是绕哪一个轴旋转任意的$\theta$角度，在其对应的3x3的旋转矩阵中，其主对角线元素的和一定是$1+2cos\theta$。这个主对角线元素的和其实叫矩阵的迹（trace）。</p>
<p>旋转矩阵的行列式的值都是1。</p>
<p>对于旋转矩阵来说：$R_i^{-1}(\theta)=R_i(-\theta)$。可以通过这种方式来求旋转矩阵的逆（当然也可以通过转置矩阵来求）。</p>
<p>对于缩放矩阵来说：$S^{-1}(s)=S(1/s_x,1/s_y,1/s_z)$。</p>
<p>把缩放矩阵对角线上的元素（即上面公式中的$s$）中的1个或3个值取反，将会得到一个镜像矩阵（reflection matrix\mirror matrix）。如果有2个值取反，将会在原来的缩放效果上旋转180度。</p>
<p>镜像矩阵会把一个三角形的顶点从逆时针绕序变成顺时针绕序，将会导致不正确的光照以及背面裁剪。检测一个矩阵是否是镜像矩阵的方式是：看其左上角3x3矩阵的行列式是否是负数，如果是负数的话，说明这个矩阵是镜像矩阵。</p>
<p>沿任意正交基向量$f^x$、$f^y$、$f^z$进行缩放的缩放矩阵如下：<br>$$<br>X=FS(s)F^T<br>$$<br>其中：<br>$$<br>F=<br>\begin{bmatrix}<br>f^x &amp; f^y &amp; f^z &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$</p>
<p>通用的剪切矩阵如下所示：<br>$$<br>M=<br>\begin{bmatrix}<br>1 &amp; h_{yx} &amp; h_{zx} &amp; 0 \\<br>h_{xy} &amp; 1 &amp; h_{zy} &amp; 0 \\<br>h_{xz} &amp; h_{yz} &amp; 1 &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$<br>剪切矩阵有点像是把矩形变成平行四边形的效果。例如下面的剪切矩阵：<br>$$<br>H_{xz}(s)=<br>\begin{bmatrix}<br>1 &amp; 0 &amp; s &amp; 0 \\<br>0 &amp; 1 &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$<br>当它作用到一个点$p$上会产生一个新的点：$(p_x+sp_z, p_y, p_z)^T$。可以看出当$s$处在第一行第三列时，对应的$H$的下标是$H_{xz}$，作用的结果是用$s$乘上$z$分量，把结果加到$x$分量上。</p>
<p>有两点值得注意：</p>
<ul>
<li>剪切矩阵行列式的值等于1。</li>
<li>剪切矩阵作用前后，作用对象的面积是不变的（想象矩形和对应的错位的四边形）。</li>
</ul>
<p>对单位法线乘上逆转置矩阵以后，法线通常不再是单位向量，需要再做一次归一化。而且由于计算4x4矩阵的逆矩阵很费时，并且平移对法线向量是没有影响的，所以通常使用左上角3x3矩阵的逆转置矩阵，来计算经过模型变换之后的法线向量。而且如果是统一缩放的话，这个模型矩阵会是一个正交矩阵，可以把求逆变成求转置。而且模型变换里如果没有缩放操作的话，法线的再次归一化都是可以不做的；或者是统一缩放的话，也可以用缩放因子来直接归一化法线向量，而不用通过传统的平方和再开方的操作。</p>
<p>常用的求逆矩阵的方法有以下几种：</p>
<ul>
<li>如果矩阵只是一系列平移或者旋转矩阵的组合，那么可以通过对参数取反，并且翻转矩阵相乘的顺序就可以了。比如$M=T(t)R(\theta)$，那么$M^{-1}=R(=\theta)T(-t)$。</li>
<li>如果矩阵是正交的，那么$M^{-1}=M^T$。</li>
<li>如果只是普通的矩阵，那么可以通过求伴随矩阵来求逆矩阵。因为逆矩阵本身等于伴随矩阵除以矩阵行列式的值，如果行列式的值等于0，那么这个矩阵实际上是奇异的，不存在逆矩阵。可以通过克莱姆法则（Cramer’s rule）、LU分解（LU decomposition）或者高斯消元（Gaussian elimination）来求逆矩阵。其中克莱姆法则更好一点，因为它的分支语句更少。</li>
</ul>
<p>和转换法线类似，如果一个逆矩阵是作用到向量而不是一个点的时候，用左上角的3x3的矩阵来求逆，会比对4x4矩阵求逆更快（矩阵的第4列实际上对一个向量是没有影响的）。</p>
<p>使用欧拉角来表示旋转，不仅会出现万向节锁的问题，而且还很难在一个欧拉角集合（绕xyz三个轴的旋转角度的集合）和另一个欧拉角集合之间进行插值。</p>
<p>对于任意一个旋转矩阵来说，有：<br>$$<br>E =<br>\begin{bmatrix}<br>e_{00} &amp; e_{01} &amp; e_{02} \\<br>e_{10} &amp; e_{11} &amp; e_{12} \\<br>e_{20} &amp; e_{21} &amp; e_{22} \\<br>\end{bmatrix}<br>=R_z(r)R_x(p)R_y(h)=<br>\begin{bmatrix}<br>cosr~cosh - sinr~sinp~sinh &amp; -sinr~cosp &amp; cosr~sinh+sinr~sinp~cosh \\<br>sinr~cosh+cosr~sinp~sinh &amp; cosr~cosp &amp; sinr~sinh-cosr~sinp~cosh \\<br>-cosp~sinh &amp; sinp &amp; cosp~cosh<br>\end{bmatrix}<br>$$<br>所以，可以从旋转矩阵反推出三个轴上的欧拉角：<br>$$<br>\begin{align}<br>&amp; h = atan2(-e_{20},e_{22}) \\<br>&amp; p = arcsin(e_{21}) \\<br>&amp; r = atan2(-e_{01},e_{11})<br>\end{align}<br>$$<br>不过，有一个例外，如果$cosp=0$，会出现万向节锁，这时$r$和$h$是绕同一个轴在转（转的方向有可能刚好相反，取决于$p$是$-\pi/2$还是$\pi/2$）。这种情况下就不能用上诉公式来算了。具体可参考《Realtime Rendering》P72，或者参考文献[1]。</p>
<p>有的时候可能想要从任意的线性变换矩阵中，分离出平移矩阵或者绕xyz轴的旋转矩阵，相关算法可以参考文献[2][3][4][5]。</p>
<p>如果我们想要求绕任意轴$r$旋转$\theta$角度的旋转矩阵。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_1.png?raw=true" alt><br>我们可以先构建一个正交基（$rst$），然后把$r$轴旋转到和x轴重合，然后在x轴上旋转$\theta$角，再把$r$轴旋转回去。一个很关键的问题是怎么构建这个正交基，我们可以通过对$r$里面绝对值最小的分量取0，再交换剩下的两个分量，并且前面的那个分量取反，来得到和$r$正交的向量$s$：<br>$$<br>\begin{align}<br>&amp;\overline{s}=<br>\begin{cases}<br>(0,-r_z,r_y), &amp; if~|r_x| \le |r_y| ~and~|r_x| \le |r_z| \\<br> (-r_z,0,r_x), &amp; if~|r_y|\le |r_x| ~and~|r_y|\le|r_z| \\<br>(-r_y,r_x,0), &amp; if~|r_z|\le |r_x| ~and~|r_z|\le|r_y|<br>\end{cases}\<br>&amp;s=\overline{s}/||\overline{s}|| \\<br>&amp;t=r\times s<br>\end{align}<br>$$<br>那么把$r$旋转到$x$轴的旋转矩阵为：<br>$$<br>M=<br>\begin{bmatrix}<br>r^T\\<br>s^T\\<br>t^T<br>\end{bmatrix}<br>$$<br>所以绕任意轴$r$旋转$\theta$角的旋转矩阵为：<br>$$<br>X=M^TR_x(\theta)M<br>$$<br>另一种绕任意轴旋转的方式是直接利用如下旋转矩阵：<br>$$<br>R=<br>\begin{bmatrix}<br>cos\theta+(1-cos\theta)r_x^2 &amp; (1-cos\theta)r_xr_y-r_zsin\theta &amp; (1-cos\theta)r_xr_z+r_ysin\theta \\<br>(1-cos\theta)r_xr_y+r_zsin\theta &amp; cos\theta+(1-cos\theta)r_y^2 &amp; (1-cos\theta)r_yr_z-r_xsin\theta \\<br>(1-cos\theta)r_xr_z-r_ysin\theta &amp; (1-cos\theta)r_yr_z+r_xsin\theta &amp; cos\theta + (1-cos\theta) r_z^2<br>\end{bmatrix}<br>$$<br>对于该矩阵可以参考文献[6]。</p>
<h2 id="四元数"><a href="#四元数" class="headerlink" title="四元数"></a><font size="5" color="red">四元数</font></h2><p>因为对于任意一个单位复数都有$cos\theta+isin\theta=e^{i\theta}$。所以对于一个四元数$\hat q$有：<br>$$<br>\hat q=sin\theta \boldsymbol{u_q}+cos\theta=e^{\theta\boldsymbol{u_q}}<br>$$<br>所以：<br>$$<br>log(\hat q)=log(e^{\theta\boldsymbol{u_q}})=\theta\boldsymbol{u_q}  \\<br>\hat q^t=(sin\theta \boldsymbol{u_q}+cos\theta)^t=e^{\theta\boldsymbol{u_q}}=sin(t\theta )\boldsymbol{u_q}+cos(t\theta)<br>$$<br>单位四元数对于旋转的意义和作用见下图：<br><img src alt><br>一个四元数$\hat q$对应的矩阵如下：<br>$$<br>M^q=<br>\begin{bmatrix}<br>1-s(q_y^2+q_z^2) &amp; s(q_xq_y-q_wq_z) &amp; s(q_xq_z+q_wq_y) &amp; 0 \\<br>s(q_xq_y+q_wq_z) &amp; 1-s(q_x^2+q_z^2) &amp; s(q_yq_z-q_wq_x) &amp; 0 \\<br>s(q_xq_z-q_wq_y) &amp; s(q_yq_z+q_wq_x) &amp; 1- s(q_x^2+q_y^2) &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$<br>其中，$s=2/||\hat q||^2$，如果$\hat q$是单位四元数，那么$s=2$。至于从旋转矩阵反推四元数，可以参考原书P80。</p>
<p>对于动画对象，使用四元数的球面线性插值可能会很有用，但是它不适用于对摄像机的朝向进行插值，因为插值过程中可能会倾斜摄像机的上向量，导致画面走形。</p>
<p>球面线性插值的公式中由于涉及到三角函数，所以计算开销实际上是很大的。对于在渲染管线中使用更加优化的方式来实现球面线性插值，可以参考文献[7][8][9][10]。</p>
<p>球面线性插值虽然可以让插值的角速度保持均匀，但是它在多个四元数之间插值时，在切换点处可能不可导，就会出现抖动问题，可以使用球面四边形插值来解决这个问题（本质上类似于样条曲线）。</p>
<p>有时候我们可能想用从一个方向$s$旋转到另一个方向$t$，而且希望旋转的路径尽可能短。这时候用四元数来实现是最方便的。先确定旋转轴$u=(s\times t)/||s\times t||$，其中$||s\times t||=sin(2\theta)$，$2\theta$是$s$和$t$绕着$u$轴的旋转角度。相应的旋转四元数可表示为$\hat q=(sin\theta u,cos\theta)$。并且有一点值得注意：$s\cdot t=cos(2\theta)$，可以用它来求$sin\theta$等等。不过对于这个四元数以及它对应的旋转矩阵有效率更高的表现形式（矩阵中没有三角函数），具体可参考原书P83以及文献[11]。当$s$和$t$的夹角接近0度或者90度时会出现一些特殊情况，具体可参见原书P84。</p>
<h2 id="顶点混合"><a href="#顶点混合" class="headerlink" title="顶点混合"></a><font size="5" color="red">顶点混合</font></h2><p>想象一下小臂和大臂分别使用不同的旋转矩阵，而肘关节实际上处于小臂和大臂的重叠区域。如下图最左边所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_3.png?raw=true" alt><br>如果小臂和大臂使用不同的旋转矩阵，动起来的时候会发现肘关节那个相当不真实。一个解决方案就是使用顶点混合（Vertex Blend，又叫线性混合蒙皮linear-blend skinning）[12][13]。动一下自己的肘关节就会发现，肘部的运动其实和大臂小臂都有关系，所以就在手臂里加入了很多骨骼（bone）（上图中间那个图里的绿线），肘关节上的点受到多个骨骼的变换矩阵的影响，上图右边的(2/3, 1/3)就表示肘关节上的那个点2/3的变换来自于小臂，1/3的变换来自于大臂，这样肘关节上的点就是小臂大臂共同作用的结果了。看下面这幅图能更明白骨骼的意义：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_4.png?raw=true" alt><br>线性混合蒙皮在数学上的表示如下：<br>$$<br>u(t)=\sum_{i=0}^{n-1}\omega_iB_i(t)M_i^{-1}p<br>$$<br>其中，$p$是原始的顶点，$u(t)$是线性混合变换之后的顶点，$M_i$是骨骼坐标系到世界坐标系的变换矩阵，$t$是时间，$\omega_i$骨骼$i$作用在顶点$p$上的权重。</p>
<p>上面的线性混合蒙皮的缺点是：有时候会出现折叠、扭曲或者自交。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_5.png?raw=true" alt><br>一个更好的解决方案是使用对偶四元数（dual quaternions）[14][15]。它的开销比较小。但是还是会出现一些隆起的现象（bulging effects），中心旋转蒙皮（center-of-rotation skinning）[16]的方法可以解决这个问题。</p>
<h2 id="模型变形"><a href="#模型变形" class="headerlink" title="模型变形"></a><font size="5" color="red">模型变形</font></h2><p>模型变形（Morphing）是指从一个模型，随着时间渐渐变成另一个模型。主要要解决的两个问题是：变形前后两个模型的顶点对应关系、以及顶点怎样随着时间进行插值。关于两个模型的顶点对应关系，可以参考一下这哥们的调查[17]。</p>
<p>至于顶点插值，可以使用简单的随着时间变化的线性插值。不过还有一种更可控的方法：如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_6.png?raw=true" alt><br>把原始的正常模型定义为$\mathcal{N}$，不同姿势下的模型定义为$\mathcal{P}_i$，然后计算这些姿势下的模型和原模型的差异$\mathcal{D}_i=\mathcal{P}<em>i-\mathcal{N}$。然后变形之后的模型$\mathcal{M}$可以通过原模型和差异模型来加权得到：<br>$$<br>\mathcal{M}=\mathcal{N}+\sum</em>{i=1}^k \omega_i\mathcal{D}_i<br>$$<br>比如上图算出来的差异模型中，只有嘴存在差异。如果给这个差异（嘴）施加一个负的权重，那么就会变成一个sad mouth。这个变形模型的方法叫做morph targets或者blend shapes[18]。下图是游戏《恶名昭彰：次子》中对角色面部使用morph targets的效果：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_7.png?raw=true" alt></p>
<h2 id="投影"><a href="#投影" class="headerlink" title="投影"></a><font size="5" color="red">投影</font></h2><p>对于正交投影来说，它实际上是把一个以(l,b,n)为左下角、以(r,t,f)为右上角的AABB长方体视锥体，通过平移缩放变成以原点为中心、半径为1的立方体。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_8.png?raw=true" alt><br>所以正交投影矩阵可以由平移和缩放矩阵来得到：<br>$$<br>P_o=S(s)T(t)=<br>\begin{bmatrix}<br>\frac{2}{r-l} &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; \frac{2}{t-b} &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; \frac{2}{f-n} &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>\begin{bmatrix}<br>1 &amp; 0 &amp; 0 &amp; -\frac{l+r}{2} \\<br>0 &amp; 1 &amp; 0 &amp; -\frac{t+b}{2} \\<br>0 &amp; 0 &amp; 1 &amp; -\frac{f+n}{2} \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{2}{r-l} &amp; 0 &amp; 0 &amp;  -\frac{r+l}{r-l} \\<br>0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b}  \\<br>0 &amp; 0 &amp; \frac{2}{f-n} &amp; -\frac{f+n}{f-n}  \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$<br>它的逆矩阵为$P_o^{-1}=T(-t)S(\frac{r-l}{2},\frac{t-b}{2},\frac{f-n}{2})$。OpenGL的正交投影矩阵通常就是上面的$P_o$，其z坐标范围在[-1,1]之间。而DirectX会把z坐标映射到[0,1]之间，相当于还要左乘一个映射矩阵：<br>$$<br>P_{o[0,1]}=<br>\begin{bmatrix}<br>1 &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; 1 &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 0.5 &amp; 0.5 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>\times P_o=<br>\begin{bmatrix}<br>\frac{2}{r-l} &amp; 0 &amp; 0 &amp;  -\frac{r+l}{r-l} \\<br>0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b}  \\<br>0 &amp; 0 &amp; \frac{1}{f-n} &amp; -\frac{n}{f-n}  \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$</p>
<h2 id="透视投影"><a href="#透视投影" class="headerlink" title="透视投影"></a><font size="5" color="red">透视投影</font></h2><p>透视投影的fov太小，会减弱透视效果；fov太大的话会导致物体变形。</p>
<p>透视投影其实也是使用如下矩阵来把一个近平面左下角为(l,b,n)、左上角为(r,t,n)的视锥体，转换成一个以原点为中心、半径为1的立方体：<br>$$<br>P_p=<br>\begin{bmatrix}<br>\frac{2n}{r-l} &amp; 0 &amp; -\frac{r+l}{r-l} &amp; 0 \\<br>0 &amp; \frac{2n}{t-b} &amp; -\frac{t+b}{t-b} &amp; 0 \\<br>0 &amp; 0 &amp; \frac{f+n}{f-n} &amp; -\frac{2fn}{f-n} \\<br>0 &amp; 0 &amp; 1 &amp; 0<br>\end{bmatrix}<br>$$<br>在OpenGL里还要对z坐标作一个镜像，DirectX里需要把z映射到[0,1]里，具体数值和上面的矩阵都有点差异，详情可参考原书P99。</p>
<p>由于深度值是非线性变换的，为了增加远距离的深度值精度，可以不在深度缓存中直接存储深度值，而是存储1.0-深度值（逆转深度，reversed z），并且是用浮点格式来存储。对于用整数来存深度、用浮点数来存深度、以及存储浮点的逆转深度值，它们的精度对比情况如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_4_9.png?raw=true" alt><br>文献[19]提出在转换顶点的过程中，把投影矩阵单独分离出来会带来更好的深度上的精度效果。也就是说用$P(Mp)$比用$(PM)p$会有很好的z精度。文献[20]中提出使用对数深度值(log)来提高Shadow map里深度值的精度。文献[21]提出使用上一帧的z-buffer来算一个最大的近平面和最小的远平面。文献[22]提出使用多个子视锥体来提高深度精度，也就是在深度方向上把原来的视锥体切成多个平截头体。</p>
<p>而之所以让NDC坐标的深度值正比于$1/p_z$，是为了让硬件实现起来更简单，并且更容易进行深度压缩（第23章会再次详述这个内容）。</p>
<h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a><font size="5" color="red">Further Reading</font></h2><p>这个<a href="[http://immersivemath.com/ila/learnmore.html](http://immersivemath.com/ila/learnmore.html">网站</a><br>)是学习线性代数的好地方。对于其他线性代数学习工具以及代码库可以参考<a href="[http://www.realtimerendering.com/](http://www.realtimerendering.com/">原书网站</a><br>)。传说关于矩阵最好的书是Farin和Hansford写的《Practical Linear Algebra: A Geometry Toolbox》。《Computer Animation: Algorithms &amp; Techniques 3》这本书是讲CG动画技术的很好的书。</p>
<p>参考文献：<br>[1] Shoemake, Ken, “Euler Angle Conversion,” in Paul S. Heckbert, ed., Graphics Gems IV, Academic Press, pp. 222–229, 1994. Cited on p. 70, 73<br>[2] Thomas, Spencer W., “Decomposing a Matrix into Simple Transformations,” in James Arvo, ed., Graphics Gems II, Academic Press, pp. 320–323, 1991. Cited on p. 72, 74<br>[3] Goldman, Ronald, “Recovering the Data from the Transformation Matrix,” in James Arvo, ed., Graphics Gems II, Academic Press, pp. 324–331, 1991. Cited on p. 74<br>[4] Goldman, Ronald, “Decomposing Linear and Affine Transformations,” in David Kirk, ed., Graphics Gems III, Academic Press, pp. 108–116, 1992. Cited on p. 74<br>[5] Shoemake, Ken, “Polar Matrix Decomposition,” in Paul S. Heckbert, ed., Graphics Gems IV, Academic Press, pp. 207–221, 1994. Cited on p. 74<br>[6] Goldman, Ronald, “Matrices and Transformations,” in Andrew S. Glassner, ed., Graphics Gems, Academic Press, pp. 472–475, 1990. Cited on p. 75<br>[7]  Malyshau, Dzmitry, “A Quaternion-Based Rendering Pipeline,” in Wolfgang Engel, ed., GPU Pro 3 , CRC Press, pp. 265–273, 2012. Cited on p. 82, 210, 715<br>[8] Li, Xin, “To Slerp, or Not to Slerp,” Game Developer, vol. 13, no. 7, pp. 17–23, Aug. 2006. Cited on p. 82<br>[9] Li, Xin, “iSlerp: An Incremental Approach of Slerp,” journal of graphics tools, vol. 12, no. 1, pp. 1–6, 2007. Cited on p. 82<br>[10] Eberly, David, “A Fast and Accurate Algorithm for Computing SLERP,” Journal of Graphics, GPU, and Game Tools, vol. 15, no. 3, pp. 161–176, 2011. Cited on p. 82<br>[11] Möller, Tomas, and John F. Hughes, “Efficiently Building a Matrix to Rotate One Vector to Another,” journal of graphics tools, vol. 4, no. 4, pp. 1–4, 1999. Also collected in [112]. Cited on p. 83, 84<br>[12] Lewis, J. P., Matt Cordner, and Nickson Fong, “Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation,” in SIGGRAPH ’00: Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, ACM Press/Addison-Wesley Publishing Co., pp. 165–172, July 2000. Cited on p. 84, 87, 90, 102<br>[13] Woodland, Ryan, “Filling the Gaps—Advanced Animation Using Stitching and Skinning,” in Mark DeLoura, ed., Game Programming Gems, Charles River Media, pp. 476–483, 2000. Cited on p. 84, 85<br>[14] Kavan, Ladislav, Steven Collins, Jiˇ r´ı ˇ Zára, and Carol O’Sullivan, “Skinning with Dual Quaternions,” in Proceedings of the 2007 Symposium on Interactive 3D Graphics and Games, ACM,pp. 39–46, Apr.–May 2007. Cited on p. 87<br>[15] Kavan, Ladislav, Steven Collins, Jirı Zára, and Carol O’Sullivan, “Geometric Skinning with Approximate Dual Quaternion Blending,” ACM Transactions on Graphics, vol. 27, no. 4, pp. 105:1–105:23, 2008. Cited on p. 87<br>[16] Le, Binh Huy, and Jessica K. Hodgins, “Real-Time Skeletal Skinning with Optimized Centers of Rotation,” ACM Transactions on Graphics, vol. 35, no. 4, pp. 37:1–37:10, 2016. Cited on p. 87<br>[17] Alexa, Marc, “Recent Advances in Mesh Morphing,” Computer Graphics Forum, vol. 21, no. 2, pp. 173–197, 2002. Cited on p. 87, 88, 102<br>[18] Kleinhuis, Christian, “Morph Target Animation Using DirectX,” in Wolfgang Engel, ed., ShaderX 4 , Charles River Media, pp. 39–45, 2005. Cited on p. 89<br>[19] Upchurch, Paul, and Mathieu Desbrun, “Tightening the Precision of Perspective Rendering,” journal of graphics tools, vol. 16, no. 1, pp. 40–56, 2012. Cited on p. 101<br>[20] Lloyd, Brandon, Logarithmic Perspective Shadow Maps, PhD thesis, Dept. of Computer Science, University of North Carolina at Chapel Hill, Aug. 2007. Cited on p. 101, 241, 242<br>[21]  Lauritzen, Andrew, Marco Salvi, and Aaron Lefohn, “Sample Distribution Shadow Maps,” in Symposium on Interactive 3D Graphics and Games, ACM, pp. 97–102, Feb. 2011. Cited on p. 54, 101, 244, 245<br>[22] Sellers, Graham, Patrick Cozzi, Kevin Ring, Emil Persson, Joel da Vahl, and J. M. P. van Waveren, SIGGRAPH Rendering Massive Virtual Worlds course, July 2013. Cited on p. 102, 868, 874, 875, 876, 879</p>
<h1 id="5-Shading-Basics"><a href="#5-Shading-Basics" class="headerlink" title="5. Shading Basics"></a><font size="6" color="orange">5. Shading Basics</font></h1><hr>
<h2 id="Light-Sources"><a href="#Light-Sources" class="headerlink" title="Light Sources"></a><font size="5" color="red">Light Sources</font></h2><p>光照强度（其实就是光线的密度）为什么是随着光源到着色点之间的距离，呈现平方衰减，可以参考下图：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_1.png?raw=true" alt></p>
<p>对于光源强度的平方距离衰减，当着色点和光源之间的距离太小时会出现奇点，Unreal引擎通过给分母加一个小的常量值来解决这个问题（$c_{light_0}$是光源在$r_0$处的亮度值）：<br>$$<br>c_{light}(r)=c_{light_0}\frac{r_0^2}{r^2+\epsilon}<br>$$<br>Unreal引擎中设置的是$\epsilon=1~cm$[1]。但是这种方法对$\epsilon$应该取多少值是没有依据的。CryEngine引擎和Frostbite引擎通过把着色点和光源之间的距离截断为一个允许的最小值，来解决这个问题：<br>$$<br>c_{light}(r)=c_{light_0}(\frac{r_0}{max(r,r_{min})})^2<br>$$<br>这个$r_{min}$是有物理依据的，它表示发光物体的物理半径大小。</p>
<p>平方距离衰减的第二个缺点是：它永远不会衰减到0，这会导致光源的半径很大。通常我们想要让距离达到某个值时，光源亮度变为0，并且在这个距离上的亮度衰减的导数也是0，这样才能保证不会突然变黑，出现硬边。一个解决方案是给平方距离衰减函数乘上一个窗口函数（windowing function），即（这里以常量值平方衰减为例）：<br>$$<br>c_{light}(r)=c_{light_0}\frac{r_0^2}{r^2+\epsilon}f_{win}(r)<br>$$<br>在Unreal引擎和Frostbite引擎中，它们用的窗口函数如下：<br>$$<br>f_{win}(r)=(1-(\frac{r}{r_{max}})^4)^{+2}<br>$$<br>其中$+2$表示截断到0再平方。下图展示了平方距离衰减函数、窗口函数、平方距离衰减函数乘上窗口函数的结果：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_2.png?raw=true" alt><br>而CryEngine出于效率考虑，只是简单地在$0.8r_{max}$和$r_{max}$之间作了一个衰减[2]。而游戏《正当防卫2》中使用了如下衰减函数来代替平方距离衰减和窗口函数，能够较好地避免奇点，也能平滑地控制光源衰减半径：<br>$$<br>f_{dist}(r)=(1-(\frac{r}{r_{max}})^2)^{+2}<br>$$<br>Unreal除了平方距离衰减函数以外，还提供了一种指数衰减方式，可以微调出很多不同的衰减曲线[3]。游戏《古墓丽影》中使用了样条编辑工具来编辑衰减曲线，这种方式能够更好地控制衰减曲线的形状[4]。</p>
<p>对于聚光灯，其实就是加了一个光源强度随方向变化的函数$f_{dir}(l)$：<br>$$<br>c_{light}=c_{light_0}f_{dist}(r)f_{dir}(l)<br>$$<br>在Frostbite中使用的$f_{dir}(l)$如下：<br>$$<br>t=(\frac{cos\theta_s-cos\theta_u}{cos\theta_p-cos\theta_u})^{\overline{+}} \\<br>f_{dir}(l)=t^2<br>$$<br>其中$\theta_s$是聚光灯的内角大小，$\theta_u$是聚光灯的外角大小，$\theta_p$是光线$l$和聚光灯主方向的夹角。<br>而浏览器中three.js图形库文件中使用的$f_{dir}(l)$如下：<br>$$<br>f_{dir}(l)=smoothstep(t)=t^2(3-2t)<br>$$<br>在《古墓丽影》中还对x、y、z三个方向使用了不同的衰减函数，来创造出闪烁的火把等等光源效果[4]。</p>
<h2 id="Implementing-Shading-Models"><a href="#Implementing-Shading-Models" class="headerlink" title="Implementing Shading Models"></a><font size="5" color="red">Implementing Shading Models</font></h2><p>需要注意的是，我们通常需要在顶点着色器输出发现之前对法线进行归一化，并且在片元着色器里接收到法线之后还要做一次归一化。因为在顶点着色器里不对法线做归一化的话，插值出来的法线会偏向于较长的法线；而即使是在单位法线之间进行线性插值，插值得到的法线也不是归一化的。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_3.png?raw=true" alt><br>而另外一些指向特定点的向量，比如光线向量、视线向量等等，如果需要插值，那么在顶点着色器里不能对它们进行归一化，否则插值出来会得到错误的向量。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_4.png?raw=true" alt></p>
<h2 id="Aliasing-and-Antialiasing"><a href="#Aliasing-and-Antialiasing" class="headerlink" title="Aliasing and Antialiasing"></a><font size="5" color="red">Aliasing and Antialiasing</font></h2><p>对于连续信号，通常会经历一个采样，再根据采样得到的样本进行重建信号的过程。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_5.png?raw=true" alt><br>对于采样频率，有一个著名的Nyquist采样定理：为了不失真地恢复模拟信号，采样频率应该大于原始模拟信号的最大频率的两倍。关于采样定理的通俗理解，可以参考<a href="https://zhuanlan.zhihu.com/p/45004323" target="_blank" rel="noopener">这篇文章</a>。<br>而想要从采样样本中重建原始信号，需要用到滤波器。常用的滤波器有：盒滤波器、三角滤波器、正弦滤波器。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_6.png?raw=true" alt><br>使用这三种滤波器来重建信号的过程如下面几幅图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_7.png?raw=true" alt><br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_8.png?raw=true" alt><br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_9.png?raw=true" alt><br>正弦滤波器（sinc filter）是一个理想的低通滤波器。因为在频率域上，一个理想的低通滤波器就是一个盒滤波器（高于某个值的频率都被抛弃掉了），而通过傅里叶理论将其转换到空间域，正好是一个正弦滤波器。下面是一个采样频率为1.0时刚好对应的正弦滤波器函数（也就是原连续信号的最大频率必须小于$1/2$）：<br>$$<br>sinc(x)=\frac{sin(\pi x)}{\pi x}<br>$$<br>实际上如果原连续信号存在有些频率大于$1/2$的部分，那么这个低通滤波器会移除这些高频部分。</p>
<p>遗憾的是由于正弦滤波器的宽度无限大并且有些地方是负的，所以实际上通常不会直接使用它。</p>
<p>我们可以通过采样得到的样本来重建出原来的连续的信号，但是连续信号无法直接被显示出来，还需要对这个重建出来的连续信号再次采样（为了放大或者缩小原来的信号）。</p>
<p>重采样（resampling）分为降采样（downsampling）和增采样（upsampling）。比如原来的采样位置是0,1,2,…，采样位置间距是1，如果重采样时采样间距比原来的大（这里是大于1），那么采样距离变大了，采样数变少了，采样频率也随之变少，就是降采样，也是在缩小信号（minification）；如果重采样时采样距离变小了，那么采样数会增加，就是增采样，也是在放大信号（magnification）。</p>
<p>下图是增采样的一个例子，只需要使用更小的间距来采样恢复出来的连续函数就可以：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_10.png?raw=true" alt><br>但是对于降采样来说，直接以更大的间距来采样会导致采样频率变小，很有可能导致采样频率不再大于连续函数最大频率的二倍，满足不了采样定理，就会出现信号失真。所以通常不是直接对恢复出来的连续函数进行降采样，而是使用一个周期更大（频率更小）的低通滤波器（比如$sinc(x/a)$）先来作用一次这个恢复出来的连续函数，去掉该连续函数中的高频部分以后，再按想要的采样间距来对它进行降采样（这时能保证满足采样定理，这个过程有点像mipmap的硬件滤波或者降采样进行高斯模糊的过程）。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_11.png?raw=true" alt></p>
<p>有一种受启发于累积缓存（accumulation buffer）的超采样（supersampling）算法，不是像原始那样渲染一张大分辨率的纹理再降采样，而是使用多张纹理来实现超采样。比如对于2x2的超采样，它就使用4张等大小的渲染纹理，对于每一张渲染纹理在x或者y方向上偏移一点点视角位置（通常是给投影矩阵施加一个小的平移量，就是相机抖动），以此来获得在一个像素内的多个不同的采样位置。从而实现超采样（只是把一张大图分成了几张小图而已）[5][6]。</p>
<p>但是上面这种算法时间和内存消耗都太大了，而TAA正是启发于此：使用前面几帧的图像来进行超采样，这就是TAA的核心思想。</p>
<p>但是这种时域反走样有几个问题：</p>
<ul>
<li>如果前面那些帧的权重不是相等的，对于静态场景会出现闪烁。</li>
<li>对于快速移动的物体，或者快速移动的相机，会出现重影（ghosting）。一个解决方案是只对低速运动的物体进行这种时域反走样。另一种解决方案是使用重投影（reprojection）来算出当前帧像素对应的世界位置，在上一帧里的像素位置，然后对这两个像素位置求一个差值，即运动向量（motion vector），以此构成速度缓存（velocity buffer，就是Motion Blur的思想）。这样的话无论物体运动多快，都可以准确找到它在上一帧画面里的位置[7]。不过在Unreal中避免重影的方法是：以当前像素为中心算一个周围像素的颜色的最大最小值，然后下一帧在算时域反走样的时候看看像素的颜色会不会超过了这个最大最小值，如果超过了就说明会出现重影（因为颜色很明显变化剧烈，就很有可能是不知名颜色飞进来了，很可能是重影），可以参考<a href="https://zhuanlan.zhihu.com/p/64993622" target="_blank" rel="noopener">这里</a>。</li>
</ul>
<p>至于在像素点内怎样选取子采样点也是很重要的一步。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_12.png?raw=true" alt><br>比如把一个像素划分成4x4的格子，其中RGSS（Rotated grid supersampling，也叫Latin hypercube或者N-rooks sampling）就是一种保证在每一行每一列都有子采样点的方案（类似于八皇后问题）。这样做的好处是人类通常对水平、垂直或者倾斜45的边上的走样最为敏感，采用RGSS能够比普通的2x2Grid在水平或者垂直方向带来更多有效的判断点。但是还是有点不足，因为可能会出现所有子采样点在一条对角线上的情况，这样对倾斜45度的边的反走样效果很不好。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_13.png?raw=true" alt><br>所以通常还想要这些子采样点不相邻，如上图右边所示。可以结合抖动、哈顿序列或者泊松盘来实现[8][9]。还可以直接用一种基础的哈顿序列来产生在空间上分布很好、没有聚类的子采样点[10]。</p>
<p>而Quincunx采样方式是一个五个子采样点的方式，如上上幅图中所示。它能够被很简单地用到TAA中[11][12]。它的好处是相邻的像素可以共用子采样点，实际上每个像素只需要算两个子采样点。另一个叫镜像四边形（FLIPQUAD）的方法，结合了RGSS和Quincunx的优点：每个像素只算两个子采样点，但是反走样效果接近Quincunx。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_14.png?raw=true" alt><br>参考文献[13]里还介绍了一些其他同样利用了共享顶点来实现的低消耗的采样方式。</p>
<p>还有一种反走样方法叫形态学反走样（morphological antialiasing，MLAA）。形态学的意思是和结构或者形状相关。它实际上是一种后处理方法，通常都是先通过边缘检测算法检测出边缘（包括几何边缘、阴影边缘、高光边缘等等），然后对这些边缘进行反走样处理。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_15.png?raw=true" alt><br>形态学反走样通常用在移动设备的环境下，因为它内存消耗少而且速度快（虽然有很多缺点）。关于形态学反走样，可以看一下这篇综述文献[13]。其中最流行的形态学反走样是FXAA(fast approximate antialiasing)和SMAA(subpixel morphological antialiasing)，因为都有跨平台的源码，而且还结合了TAA~。</p>
<h2 id="Transparency-Alpha-and-Compositing"><a href="#Transparency-Alpha-and-Compositing" class="headerlink" title="Transparency, Alpha, and Compositing"></a><font size="5" color="red">Transparency, Alpha, and Compositing</font></h2><p>一个比较简单的效率很高的透明算法是：基于点阵的像素剔除透明（screen-door transparency），原理是让一个4x4（或者更大）的像素块里的像素具有不同的Alpha值，然后剔除掉那些小于阈值的像素，以此来模拟透明。可参见参考文献[14]，或者<a href="https://digitalrune.github.io/DigitalRune-Documentation/html/fa431d48-b457-4c70-a590-d44b0840ab1e.htm" target="_blank" rel="noopener">这里</a>、<a href="http://walkingfat.com/stipple-transparency-%e7%82%b9%e9%98%b5%e5%83%8f%e7%b4%a0%e5%89%94%e9%99%a4%e5%8d%8a%e9%80%8f%e6%98%8e/" target="_blank" rel="noopener">这里</a>。用这种方法来实现透明的优点是很简单，但是缺点也很明显：会有明显的像素化。文献[15]中提出了使用子像素结合噪声纹理来改善这个问题。好处是能够把其他一些问题（比如反走样）一并解决掉，缺点是需要更多的子像素以及内存。</p>
<p>使用Alpha Blend来进行透明混合时，薄纱和塑料的透明效果是不一样的。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_16.png?raw=true" alt><br>前者使用常用的Alpha加权混合（即$c_o=\alpha_sc_s+(1-\alpha_s)c_d$，也叫over blending，混合之后的alpha值是类似的：$\alpha_o=\alpha_s+(1-\alpha_s)\alpha_d$，通常我们设置的BackBuffer的alpha值都是1，比如(0,0,0,1)，那么在over blending过程中目标的alpha值会一直是1，这个其实无所谓了，over blending主要跟源的alpha值相关），效果会比较接近，但是后者直接把两个颜色相乘得到的结果会更好。因为塑料这种透明物体材质透光更少，会让它后面的物体看起来更暗，并且两者阴影也是不一样的效果。</p>
<p>另一种直接加的混合additive blending是$c_o=\alpha_sc_s+c_d$，这种混合方式更适用于发光效果，比如闪电或者火花，它不会减弱原来像素的亮度，而只是照亮它们。</p>
<p>比较早的一个顺序无关的透明技术是depth peeling（深度剥离），它的思想是：首先开启深度测试渲染得到离摄像机最近的一层透明表面，把渲染得到的深度值存到深度缓存里，把渲染得到的颜色值存到颜色缓存里；然后把场景再渲染一次，这次只渲染那些离得更近的片元（也就是如果发现片元的深度比深度缓存里的值还小，就抛弃它们），依然开启深度测试，那么这一次就会渲染得到离摄像机第二近的那层表面，把这次渲染得到的深度值覆盖到深度缓存里，并且这次把渲染得到的颜色值和颜色缓存里的值进行混合；以此类推，在深度上一层一层地剥离透明物体，最终得到透明效果。很明显它是从前往后进行混合的，所以需要改用这个混合公式：$c_o=\alpha_dc_d+(1-\alpha_d)\alpha_sc_s$（记住alpha是指的不透明度就很好理解这个公式了。这种从前往后混合也叫under blending，混合之后的alpha值是类似的：$\alpha_o=\alpha_d+(1-\alpha_d)\alpha_s$，由于需要从前往后绘制需要用到目标的alpha值，所以一开始清屏的alpha值不再会是1，所以从公式上就可以得到目标的alpha值会随着混合变的越来越大，而通常离摄像机越近的透明表面是越重要的，也印证了Depth Peeling更适合从前往后绘制）。depth peeling最早见参考文献<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.9286&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[14]</a>。Depth Peeling的缺点也很明显：需要把场景中的透明物体绘制很多遍，效率低。Dual Depth Peeling[15]对它作了一点改进，每遍Pass同时剥离出最近和最远的层，这样一来对透明物体的绘制Pass数量减少了一半。文献[16]里使用了一种桶排序（bucket sort）的方法，在一个Pass里能够剥离出来32层，不过有很明显的缺点，需要很大的内存来对这些层进行排序，如果再用上MSAA等等，那内存消耗会是个天文数字~。</p>
<p>1984年，Carpenter提出了如今最常用的OIT算法[16]，通过使用A-Buffer，其实就是在GPU上为每个像素创建一个链表，链表里存储了会覆盖到该像素的那些透明片元。然后通过一遍后处理，来从每个像素关联的链表里，取出这些透明片元，对它们排个序，把这些透明片元从后往前进行混合渲染。</p>
<p>还有一类透明算法是不考虑透明物体的远近，而是直接使用alpha值来进行加权混合[17]。它的计算公式是：$C_o=\sum_{i=1}^{n}(\alpha_ic_i)+c_d(1-\sum_{i=1}^{n}\alpha_i)$。其中$c_i$和$c_d$和之前有点不太一样，$c_i$和$\alpha_i$表示n个透明物体的颜色和不透明度，而$c_d$表示场景不透明物体的颜色。但是在这个公式中两个加和都有可能超过1，会出现我们不想要的混合结果，可以使用下面的加权平均公式来代替：<br>$$<br>\mu= (1-\frac{\sum_{i=1}^{n}\alpha_i}{n})^n\\<br>c_o=(1-\mu)\frac{\sum_{i=1}^n\alpha_ic_i}{\sum_{i=1}^n\alpha_i}+\mu c_d<br>$$<br>不过上面这类透明渲染方法比较适用于接近不透明或者完全透明的情况，但是如果两个透明物体的alpha是相同的，由于没有渲染顺序，用这类算法还是很可能出现问题的。文献[18][19]中把透明物体表面离摄像机的距离也加了进来，距离越近的透明表面有更大的权重。这样会得到一个更好的透明效果。并且在文献[20][21]中作者将该算法做了进一步延伸，来实现一种透色颜色（transmission color）的效果。</p>
<p>文献[22][23][24]中提供了对透明渲染算法的综述以及和其他效果进行结合的示例。</p>
<h2 id="Display-Encoding"><a href="#Display-Encoding" class="headerlink" title="Display Encoding"></a><font size="5" color="red">Display Encoding</font></h2><p>有一点需要再次解释下：在Shader里计算出来的值是在线性空间里的，但是它被显示到屏幕上时，会被显示设备乘上一个2.2次幂，为了让显示器显示出线性变化的亮度（即让显示器显示出我们在Shader里真正输出的值），需要在Shader输出结果之前，先做一个Gamma校正，即对结果值乘上一个1/2.2次幂。而对美术做出来的漫反射颜色等图片，他们在用软件做图的时候是按照线性感知在做的，即美术会觉得一个颜色是另一个颜色的亮度的2倍，这是一个线性感知，由于显示器的Gamma变换的存在，真实存在纹素里的值并不是2倍关系，但是我们再Shader里实际上想拿到的是美术感知的线性变化的值，所以在Shader里需要对从sRGB纹理中获取到的纹素值，做一个Gamma变换，即给它乘上2.2次幂，将其变换成线性空间下的值（因为显示器本身就是做的2.2次幂，来达到美术感知的线性感觉）。</p>
<p>不做Gamma校正很可能会导致边界变形（roping，即扭曲成一圈一圈的样子），如下面两幅图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_17.png?raw=true" alt><br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_5_18.png?raw=true" alt></p>
<h2 id="Further-Reading-1"><a href="#Further-Reading-1" class="headerlink" title="Further Reading"></a><font size="5" color="red">Further Reading</font></h2><p>文献[25]里深入讨论了采样模式和反走样，文献[26]里有很多不同的采样模式的生成方法。文献[27][28]里总结了很多反走样技术以及他们的优缺点、性能等等问题。文献[29]里介绍了多种形态学反走样（morphological antialiasing）方法，文献[30]里介绍了游戏中使用的一些最新的形态学反走样和时域反走样（ temporal antialiasing）方法。</p>
<p>关于半透明算法，文献[31]和文献[32]是很好的资料。文献[33]介绍了目前最先进的反走样技术。文献[34]很好地总结了有关Gamma校正的问题。</p>
<p>参考文献：<br>[1] Karis, Brian, “Real Shading in Unreal Engine 4,” SIGGRAPH Physically Based Shading in Theory and Practice course, July 2013. Cited on p. 111, 113, 116, 325, 336, 340, 342, 352, 355, 383, 385, 388, 421, 423<br>[2] Schulz, Nicolas, CRYENGINE Manual, Crytek GmbH, 2016. Cited on p. 111, 113, 631<br>[3] Unreal Engine 4 Documentation, Epic Games, 2017. Cited on p. 114, 126, 128, 129, 262, 287, 364, 611, 644, 920, 923, 932, 934, 939<br>[4] Lacroix, Jason, “Casting a New Light on a Familiar Face: Light-Based Rendering in Tomb Raider,” Game Developers Conference, Mar. 2013. Cited on p. 114, 116<br>[5] Haeberli, P., and K. Akeley, “The Accumulation Buffer: Hardware Support for High-Quality Rendering,” Computer Graphics (SIGGRAPH ’90 Proceedings), vol. 24, no. 4, pp. 309–318, Aug. 1990. Cited on p. 139, 529, 537, 547<br>[6] Mammen, Abraham, “Transparency and Antialiasing Algorithms Implemented with the Virtual Pixel Maps Technique,” IEEE Computer Graphics &amp; Applications, vol. 9, no. 4, pp. 43–55, July 1989. Cited on p. 139, 154<br>[7] Wronski, Bartlomiej, “Temporal Supersampling and Antialiasing,” Bart Wronski blog, Mar.15, 2014. Cited on p. 143, 540<br>[8] Pharr, Matt, Wenzel Jakob, and Greg Humphreys, Physically Based Rendering: From Theory to Implementation, Third Edition, Morgan Kaufmann, 2016. Cited on p. 136, 144, 145, 165, 271, 442, 445, 512, 589, 623, 630<br>[9] Teschner, Matthias, “Advanced Computer Graphics: Sampling,” Course Notes, Computer Science Department, University of Freiburg, 2016. Cited on p. 144, 165<br>[10] Karis, Brian, “High Quality Temporal Supersampling,” SIGGRAPH Advances in Real-Time Rendering in Games course, Aug. 2014. Cited on p. 142, 143, 144, 620<br>[11] Jimenez, Jorge, “Dynamic Temporal Antialiasing in Call of Duty: Infinite Warfare,” SIGGRAPH Advances in Real-Time Rendering in Games course, Aug. 2017. Cited on p. 142, 143, 145, 146, 148, 166, 805<br>[12] Sousa, Tiago, “Anti-Aliasing Methods in CryENGINE,” SIGGRAPH Filtering Approaches for Real-Time Anti-Aliasing course, Aug. 2011. Cited on p. 145, 531<br>[13] Reshetov, Alexander, and Jorge Jimenez, “MLAA from 2009 to 2017,” High-Performance Graphics research impact retrospective, July 2017. Cited on p. 143, 146, 148, 165<br>[14] Everitt, Cass, “Interactive Order-Independent Transparency,” NVIDIA White Paper, May 2001. Cited on p. 154<br>[15] avoil, Louis, and Kevin Myers, “Order Independent Transparency with Dual Depth Peeling,” NVIDIA White Paper, Feb. 2008. Cited on p. 155, 157<br>[16] Carpenter, Loren, “The A-Buffer, an Antialiased Hidden Surface Method,” Computer Graphics (SIGGRAPH ’84 Proceedings), vol. 18, no. 3, pp. 103–108, July 1984. Cited on p. 155, 626<br>[17] Meshkin, Houman, “Sort-Independent Alpha Blending,” Game Developers Conference, Mar. 2007. Cited on p. 156<br>[18] McGuire, Morgan, and Louis Bavoil, “Weighted Blended Order-Independent Transparency,” Journal of Computer Graphics Techniques, vol. 2, no. 2, pp. 122–141, 2013. Cited on p. 158<br>[19]  McGuire, Morgan, “Implementing Weighted, Blended Order-Independent Transparency,” Casual Effects blog, Mar. 26, 2015. Cited on p. 158, 569<br>[20] McGuire, Morgan, “Fast Colored Transparency,” Casual Effects blog, Mar. 27, 2015. Cited on p. 158<br>[21] McGuire, Morgan, and Michael Mara, “Phenomenological Transparency,” IEEE Transactions of Visualization and Computer Graphics, vol. 23, no.5, pp. 1465–1478, May 2017. Cited on p. 158, 623, 624, 629, 632, 649<br>[22] Wyman, Chris, “Exploring and Expanding the Continuum of OIT Algorithms,” in Proceedings of High-Performance Graphics, Eurographics Association, pp. 1–11, June 2016. Cited on p. 156, 159, 165<br>[23] Maule, Marilena, Jo˜ ao L. D. Comba, Rafael Torchelsen, and Rui Bastos, “A Survey of Raster-Based Transparency Techniques,” Computer and Graphics, vol. 35, no. 6, pp. 1023–1034, 2011. Cited on p. 159<br>[24] McGuire, Morgan, “Peering Through a Glass, Darkly at the Future of Real-Time Transparency,” SIGGRAPH Open Problems in Real-Time Rendering course, July 2016. Cited on p. 159, 165, 623, 649<br>[25] Pharr, Matt, Wenzel Jakob, and Greg Humphreys, Physically Based Rendering: From Theory to Implementation, Third Edition, Morgan Kaufmann, 2016. Cited on p. 136, 144, 145, 165, 271, 442, 445, 512, 589, 623, 630<br>[26] Teschner, Matthias, “Advanced Computer Graphics: Sampling,” Course Notes, Computer Science Department, University of Freiburg, 2016. Cited on p. 144, 165<br>[27] Drobot, Micha l, “Hybrid Reconstruction Anti Aliasing,” SIGGRAPH Advances in Real-Time Rendering in Games course, Aug. 2014. Cited on p. 141, 142, 146, 165<br>[28] Drobot, Micha l, “Hybrid Reconstruction Antialiasing,” in Wolfgang Engel, ed., GPU Pro 6 , CRC Press, pp. 101–139, 2015. Cited on p. 141, 146, 165<br>[29] Jimenez, Jorge, Diego Gutierrez, et al., SIGGRAPH Filtering Approaches for Real-Time Anti-Aliasing course, Aug. 2011.<br>[30] Reshetov, Alexander, and Jorge Jimenez, “MLAA from 2009 to 2017,” High-Performance Graphics research impact retrospective, July 2017. Cited on p. 143, 146, 148, 165<br>[31] McGuire, Morgan, “Peering Through a Glass, Darkly at the Future of Real-Time Transparency,” SIGGRAPH Open Problems in Real-Time Rendering course, July 2016. Cited on p. 159, 165, 623, 649<br>[32] Wyman, Chris, “Exploring and Expanding the Continuum of OIT Algorithms,” in Proceedings of High-Performance Graphics, Eurographics Association, pp. 1–11, June 2016. Cited on p. 156, 159, 165<br>[33] Jimenez, Jorge, “Dynamic Temporal Antialiasing in Call of Duty: Infinite Warfare,” SIGGRAPH Advances in Real-Time Rendering in Games course, Aug. 2017. Cited on p. 142, 143, 145, 146, 148, 166, 805<br>[34] Gritz, Larry, and Eugene d’Eon, “The Importance of Being Linear,” in Hubert Nguyen, ed., GPU Gems 3, Addison-Wesley, pp. 529–542, 2007. Cited on p. 161, 166, 184</p>
<h1 id="6-Texturing"><a href="#6-Texturing" class="headerlink" title="6. Texturing"></a><font size="6" color="orange">6. Texturing</font></h1><hr>
<h2 id="The-Texturing-Pipeline"><a href="#The-Texturing-Pipeline" class="headerlink" title="The Texturing Pipeline"></a><font size="5" color="red">The Texturing Pipeline</font></h2><p>在面与面的接缝处，通常会出现纹理匹配的问题，文献【1】【2】中讨论了一种在两个面之间进行混合的技术。</p>
<p>文献【3】中提出了一种把模型映射到多个立方体的投影方法（有点像体素化），叫做polycube maps。</p>
<p>关于网格展开（unwrapping），它其实也是网格参数化（mesh parameterization）的一部分，这方面可以参考文献【4】。</p>
<p>可以用半透明纹理来绘制线条，来模拟下雨效果。</p>
<h2 id="Image-Texturing"><a href="#Image-Texturing" class="headerlink" title="Image Texturing"></a><font size="5" color="red">Image Texturing</font></h2><p>如果在片元着色器中，自己生产纹理采样坐标或者改动了从顶点着色器传过来的纹理坐标，都会导致dependent texture read，会导致GPU无法预先读取(prefetch)纹理数据（和CPU利用局部缓存的Cache很像），从而导致性能下降。</p>
<h3 id="Magnification"><a href="#Magnification" class="headerlink" title="Magnification"></a><font size="4" color="green">Magnification</font></h3><p>双线性插值的计算方式如下面两幅图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_1.png?raw=true" alt><br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_2.png?raw=true" alt><br>其中先又$u$插值计算得到两个绿色的点，再由$v$插值计算得到最终红色的采样点的值。公式中$t$是个纹理采样函数，类似于<code>texture2D</code>。值得注意的是，有一个对称性：右上角的像素的影响系数等于采样点和左下角的点构成的四边形的面积，这是由于插值公式$u$是影响前半部分，$1-u$是影响后半部分的原因引起的。</p>
<p>贴图被应用到一个超过它尺寸的物体上时（即多个像素会对应到同一个纹素），称之为放大（magnification，纹素被放大了）。这个时候通常会用到最邻近插值或者上面的双线性插值。总所周知，最邻近插值会导致明显的像素块，而双线性插值实际上是个低通滤波器，会导致结果变得更模糊，丢失高频细节。为了缓解这种低频模糊，常用的解决方案是搭配使用细节纹理（detail texture）。这些细节纹理通常表示手机上的划痕、地形上的灌木等等，使用高频的repetitive模式的细节纹理，搭配低频的magnification的纹理，其效果类似于直接使用一张高分辨率纹理。</p>
<p>不是任何情况下使用双线性插值都是好的。比如下图，其中每一个格子对应一个像素：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_3.png?raw=true" alt><br>可以看到使用线性插值依然会有像素块，使用双线性插值会出现很多不想要的灰度，为了改善双线性插值的结果，需要对采出来的颜色进行重新映射：小于0.4的强制输出黑色，大于0.6的强制输出白色，在0.4和0.6之间的会被拉伸（感觉可能就是颜色不变）。从而达到上图右边的结果。</p>
<p>很多比线性滤波更高级的滤波器，可以用重复的线性插值来实现【5】。</p>
<p>比双线性插值更好是，可以在一个2x2的像素块里，使用一些曲线函数来插值。常用的曲线函数有平滑曲线（smoothstep curve）和五次多项式曲线（quintic curve）。它们的公式以及曲线图如下：<br>$$<br>s(x)=x^2(3-2x)  \\<br>q(x)=x^3(6x^2-15x+10)<br>$$<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_4.png?raw=true" alt><br>这两曲线对于把一个值平滑变换到另一个值很有用。smoothstep曲线在0和1的两端的一阶导数是0，即$s’(0)=s’(1)=0$，而quintic曲线不止于此，它的二阶导数还等于0：$q’’(0)=q’’(1)=0$。</p>
<p>这些曲线应用到插值上的方式是：用原来的纹理坐标作为参数$x$，用这些曲线来计算新的纹理坐标，然后用这些新的纹理坐标来进行硬件的双线性插值（就是把新的纹理坐标传递给<code>texture2D</code>函数）。</p>
<h3 id="Minification"><a href="#Minification" class="headerlink" title="Minification"></a><font size="4" color="green">Minification</font></h3><p>当一个像素对应多个纹素时，称之为缩小（minification，纹素被缩小了）。对缩小的滤波方式同样有最邻近和双线性插值。使用最邻近也会出现很明显的走样，而双线性插值会对相邻的2x2个纹素进行混合，效果比最邻近好很多，但是一旦一个像素对应的纹素多余四个时，双线性插值也还是会有走样。解决办法就是去增加采样频率或者降低纹理的频率。比如一张黑白相间的棋盘格纹理（相间的黑色和白色都是刚好占一个纹素），它的波长是两个纹素，那么频率就是$\frac{1}{2}$，所以为了把这张纹理显示到屏幕上，采样频率至少要是$2\times \frac{1}{2}$，也就是说需要一个纹素对应一个像素。所以，通常我们都应该尽量保证一个纹素对应一个像素来避免走样。下图是在缩小时，使用不同的滤波方法带来的效果：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_5.png?raw=true" alt><br>最常用的纹理反走样方法是我们熟知的mipmapping。</p>
<p>在生成mipmap时，要注意确保是在线性空间下进行滤波的，否则有可能会让物体看起来更暗，并且丢失很多细节和对比度。不过大多数生成mipmap的API都有正确支持sRGB纹理。</p>
<p>为了衡量一个像素占多少个纹素，用$d$来表示这个覆盖率，这个$d$是用来决定去哪些mipmap层级采样的关键因素。有人提出说把像素投影到纹理中形成一个矩形，然后使用矩形的最长边的大小，来近似这个覆盖率；另外一种说法是使用$\frac{\partial u}{\partial x}$、$\frac{\partial v}{\partial x}$、$\frac{\partial u}{\partial y}$、$\frac{\partial v}{\partial y}$中最大的绝对值来表示覆盖率$d$。比如$\frac{\partial u}{\partial x}$表示纹理坐标$u$沿着像素坐标$x$的变化量。硬件会用这个$d$来决定去哪些层采样，以尽可能实现像素和纹素1：1的比例，从而满足采样定理。大致趋势是：像素包含的纹素越多、d越大时，会采样到更模糊的mipmap层级。对于采样坐标$(u,v,d)$，比如$d$是4.7，那么硬件会先去地4层和5层按照$(u,v)$进行双线性插值，然后再根据$d$在这两层之间进行线性插值。</p>
<p>mipmap有一个最主要的缺点是：有时候会造成过度模糊（overblurring）。比如有时候会出现一个像素在$v$方向上会覆盖较多纹素，而在$u$方向上会覆盖较少纹素，但是覆盖率是按照较大值来计算的，导致像素在$u$方向上的采样层级大于它应该去采的层级，从而导致在$u$方向上变的太模糊了。如上图中间那副图所示。在$u$方向和$v$方向上，由于采的是同一层mipmap，所以换算到最精细层，其实就是采了一个正方形区域。而$u$方向和$v$方向上的覆盖率相差很大时，其实我们更想采的是一个长方形区域，覆盖率大的那个方向上多采一些（也就是层级更高一些，更模糊一些）。</p>
<p>Summd-Area Table（SAT）【6】就是一种各项异性的minification滤波方法，它实际上采的是一个长方形。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_6.png?raw=true" alt><br>它首先会创建一个和原纹理相同尺寸的辅助纹理，不同的是辅助纹理里每个通道至少需要16bit。对每一个纹素，它会去计算纹理原点到当前纹素所构成的矩形区域内，所有纹素的和，记为$s$，然后把这个和存储到辅助纹理的对应位置上。从这里就可以看出，这个辅助纹理就是区域加和表（SAT）。然后再把每个像素投影回纹理上，计算出它覆盖的那些纹素的包围盒。然后根据包围盒的顶点使用下诉公式来计算像素的颜色（准确的说应该是像素去纹理中采样出来的颜色）：<br>$$<br>c=\frac{s[x_{ur},y_{ur}]-s[x_{ur},y_{ll}]-s[x_{ll},y_{ur}]+s[x_{ll},y_{ll}]}{(x_{ur}-x_{ll})(y_{ur}-y_{ll})}<br>$$<br>公式很好理解，就像是在算面积一样~。使用SAT滤波方式代替mipmap，从纹理里最终采样出来的结果如前一幅图中最下面那张图所示。但是可以看到对角线上还是有点太模糊了。这是因为像素被投影到纹理的对角线上，想象一下上面那张图，如果像素投影到纹理上的区域沿着对角线顶到了纹理的四个边，那么它的包围盒就是整个纹理，导致很多离得很远的像素也被一起滤波进来了，最终导致这个像素采出来的颜色很糊。</p>
<p>SAT是各向异性滤波（anisotropic filtering）算法【7】中的一种，即它们是从非正方形区域里采样纹素值的。而且SAT是可以在GPU上实现的【8】，它还可以被用来改善光泽反射（glossy reflections）【9】【10】、景深（depth of field）【11】、shadow map【12】等等。但是SAT由于算的是一个平行于uv轴的包围盒，所以它会出现上面所述的对角线会很模糊的情况。故此还有一类无约束的各向异性滤波（Unconstrained Anisotropic Filtering）算法，它的包围盒可以是任意轴向的。下面是概述。</p>
<p>对于目前的图形硬件，改善纹理滤波质量最常用的方法还是重复利用硬件的mipmap。其基本原理就是把像素反投影回纹理，形成一个四边形，然后对纹理上的这个四边形区域进行多次采样，再把这些采样样本以某种算法结合在一起，构成最终的采样点颜色。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_7.png?raw=true" alt><br>这类算法会用短边来算覆盖率$d$，而之前说的mipmap通常会选择长边来算$d$，用短边的优势是采的mipmap层级没有那么模糊，然后这类各向异性算法会过四边形中点创建一条平行于长边的线（称为line of anisotropy）。当长边和短边的比例介于1:1和2:1之间时，会在这两条线上取两个采样点。这个比例越高，在线上的采样点也就越多。这种方案可以让四边形朝着任意轴向，并且会保证在长边方向上采样点数量更多，这样能避免过度模糊，并且不需要像SAT那样需要额为的辅助纹理。</p>
<p>还有一些高质量的软件采样滤波算法，比如椭圆内加权平均（Elliptical Weighted Average，EWA）滤波，该算法会把像素的影响区域反投影到纹理上，形成一个椭圆区域，然后使用一个滤波kernel来对椭圆内的纹素进行加权平均【13】。文献【14】提供了在GPU上实现EWA算法的Shader代码。</p>
<h3 id="Volume-Textures"><a href="#Volume-Textures" class="headerlink" title="Volume Textures"></a><font size="4" color="green">Volume Textures</font></h3><p>大多数GPU都支持体积纹理的mipmap。在二维纹理的不同mipmap层进行过滤，需要用到三线性插值，所以自然地三维纹理的mipmap层间过滤需要用到四线性插值（先用三线性插值在三维纹理的某一个mipmap层级中进行过滤，然后再用一个线性插值在不同的mipmap层间进行插值）。</p>
<p>当然体积纹理需要更多的内存，滤波开销也更大，但是它有其独特的优势。最直观的一点，使用体积纹理可以直接存储三维结构信息，而不需要把三维模型展开到二维平面，这种展开通常都会产生变形或者接缝（比如lightmap）。</p>
<p>而且由于体积纹理中的很多纹素其实是空的，几乎不会被采样到，所以文献【15】【16】中提出了使用稀疏八叉树（sparse octree）来存储体积纹理的纹理数据，以此来降低所需要的内存空间。文献【17】中提出了在GPU上实现八叉树的方案。文献【18】中提出了一种算法，能够把稀疏体积纹理数据打包到一个很小的纹理中。</p>
<h3 id="Texture-Representation"><a href="#Texture-Representation" class="headerlink" title="Texture Representation"></a><font size="4" color="green">Texture Representation</font></h3><p>使用图集（texture atlas）可以减少切换纹理带来的开销。不过需要注意图集的mipmap生成方式，因为原生的mipmap是把四个像素合成一个，这样的话在比较高的mipmap层级里，原来不同的图片很可能会混在一起。文献【19】中提出给每个子纹理单独创建mipmap层级，然后再把这些子纹理的层级合并到图集里。文献【20】中提出了一种算法，结合表面参数化来优化mipmap的生成。文献【21】中提出了一种名为Ptex的系统，它给每个细分表面的每个四边形都分配了一个小纹理，说是可以避免接缝（Seam）（暂时不知道怎么做的~，神奇）。不过图集除了前面说的子纹理溢出的问题，还很难支持repeat、wrapping等纹理环绕方式。</p>
<p>另一种更加单的减少切换纹理开销的方式是直接使用纹理数组，为纹理数组中的每个纹理都单独生成mipmap，这样就能直接避免高层级mipmap发生纹理溢出，并且还能支持repeat等纹理环绕方式。使用纹理数组比单个单个去切换绑定的纹理，速度要快5倍【22】。</p>
<p>其实，支持非绑定纹理的API，可以从本质上避免这种因为切换纹理带来的状态切换开销。而且非绑定纹理是没有绑定纹理数量的上限的，因为每个纹理仅仅64位的指针相关联，这个指针指向这个纹理的实际数据结构所在的内存位置。而应用程序端只需要确保纹理是在GPU上的即可。非绑定纹理可以避免驱动器在绑定纹理时产生的开销，能够加快渲染速度。</p>
<h3 id="Texture-Compression"><a href="#Texture-Compression" class="headerlink" title="Texture Compression"></a><font size="4" color="green">Texture Compression</font></h3><h4 id="Block-Compression"><a href="#Block-Compression" class="headerlink" title="Block Compression"></a><font size="3" color="blue">Block Compression</font></h4><p>不同的图像文件格式（如JPEG、PNG）有着不同的图像压缩方式，但是它们解码的开销都有点大。S3公司提出了名为S3 Texture Compression（S3TC）的压缩方案【23】，后来发展成为行业标准（在DX9中叫DXTC，在DX10以上改叫BC（Block Compression））。它会把图像压缩成很多独立编码的固定大小的块，这样方便地快速解码。每个块之间相互独立，不存在任何依赖。它会对4x4的纹素块（也叫tile）进行独立编码。编码方式是基于插值的。它会把4x4的纹素块变成两个基准颜色，并且每个纹素中记录了一个插值系数。这样一来每个纹素可以用它的插值系数以及前面说的两个基准颜色，来近似恢复出纹素原来的颜色（像调色板一样）。这种块压缩方案有7个不同的变种，每种方案的具体压缩细节都不太一样：</p>
<ul>
<li>BC1（DXT1）：它给每个4x4的块记录两个基准颜色，每个颜色的格式是R5G6B5（BC1不考虑alpha通道），并且每个纹素用2个bit来记录它的插值系数。那么解压时每个纹素的颜色可以计算为：$c=(1-\gamma)c_0+\gamma c_1$。其中$c_0$、$c_1$是两个基准颜色，$\gamma$是每个纹素的插值系数，由于是2bit表示的，所以其取值有：0，1/3，2/3，1。把这四个$\gamma$值带入刚才的公式，就可以解压近似恢复出每个纹素的颜色值。这样一来，4x4的纹素块可被压缩成$(5+6+5)<em>2+2</em>16=64bit$，而未压缩之前是$8<em>3</em>16=384bit$，所以压缩比是$384:64=6:1$。</li>
<li>BC2（DXT3）：在BC1的基础上，为每个纹素多分配4bit来表示alpha值。</li>
<li>BC3（DXT5）：在BC1的基础上，对alpha通道使用和RGB一样的编码方式。对每个4x4的块记录两个基准alpha值（用8bit表示），然后对每个纹素再用3bit来记录一个对alpha值的插值系数，即alpha的可能取值为0/7~7/7。</li>
<li>BC4：它是对颜色值的单个通道进行编码，编码方式和BC3一样。比如对R通道，对整个块记录两个8bit的基准R颜色值，再对每个纹素使用3bit来记录插值系数。</li>
<li>BC5：它是对颜色值的两个通道进行编码，编码方式和BC3一样。BC3是对RGB通道按基准色和系数进行编码，而BC5是对RG两个通道按同样的方式进行编码（BC3~BC5编码方式一样，应该只是用途不一样，针对不同通道数量的纹理而已）。</li>
<li>BC6H：由于BC1~BC5是对颜色的线性拟合，所以如果4x4纹素块里刚好有红、绿、蓝这三个颜色，是不可能通过线性插值能恢复出来的，必然会有一个颜色会丢失。而BC6H把4x4的纹素块进一步分成了各种不同的子集，相当于可以拟合出多条不同的线，而不是像之前一样只用一条直线来进行插值集合，这样编码得到的误差比较小，当然运算量也很大。具体可以参考<a href="http://www.klayge.org/2015/03/04/bc7%E7%9A%84%E5%BF%AB%E9%80%9F%E5%8E%8B%E7%BC%A9%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9Abc7%E7%9A%84%E7%BB%93%E6%9E%84/" target="_blank" rel="noopener">这里</a>。BC6H主要是用来压缩HDR浮点纹理的。</li>
<li>BC7：它主要是用来更精准地压缩LDR纹理的，比如RGB或者RGBA格式的纹理，能够得到高质量的压缩结果。</li>
</ul>
<h4 id="ETC"><a href="#ETC" class="headerlink" title="ETC"></a><font size="3" color="blue">ETC</font></h4><p>OpenGL ES中集成了另一种压缩算法，叫Ericsson texture compression（ETC）【24】。它的解压速度也很快。它首先对整张纹理设置了一个亮度编码表，表里总共有16列，每列4个亮度值。然后会对2x4或者4x2的块（取决于哪一个能获得更好的压缩质量）存储一个基准颜色（格式是R4G4B4），而且对整个块记录了一个4bit的索引值，用来去索引表里的某一列。同时块里的每个纹素记录了一个2bit的索引值，用来从块对应的列中取出一个亮度值。解码时把这个亮度值加到基准颜色上，作为纹素的颜色。<br>示例如下：<br>比如块的基准颜色是（0,2,15），二进制是(0000,0010,1111)，解码时需要扩展到8bit，就是复制一下即可：(0000000,00100010,11111111)，颜色值变为(0,34,255)。块的4bit索引值是8，那么就去亮度编码表中取出第9列的四个亮度值（比如是（-16,-4,4,16）），如果块里某个纹素的2bit索引值是2，那么就去拿到第3个亮度值4，那么块里的这个纹素解码出来的颜色是：(0,34,255)+(4,4,4)=(4,38,259)；再clamp到255为(4,38,255)。这个压缩思想其实就是把图像的颜色信息和亮度信息进行剥离，再合回来。如下图所示（中间的灰度图其实就是亮度信息）：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_8.png?raw=true" alt><br>可以看到对于4x2的块，ETC最终需要存储$4<em>3+4+2</em>8=32bit$，而压缩之前需要$8<em>3</em>4*2=192$，压缩比为$192:32=6:1$，可以看到压缩比和DXT1一样，压缩质量也和DXTC相当。</p>
<p>ETC还有一个优化版本（基于差值的ETC编码）：把两个4x2（或者2x4）的块放到一起变成4x4的块，如果这两个4x2的块的颜色相差很小，就可以对其中一个块使用5个bit来表示它的基准颜色（即R5G5B5），再使用3个bit来表示另一个块的基准颜色和它的差值（即R3G3B3，因为差值很小所以可以用更小的bit来表示），那么另一个块的基准颜色就可以用前一个块5bit的基准颜色和这个3bit的差值相加得到，这样一来两个块的基准颜色都能达到5个bit的精度。并且亮度编码表从16列变成了8列，所以每个4x2的块只需要3bit的索引值来索引这个表，然后块里每个纹素记录两个bit来索引表里某一列的某个亮度值。总结下一，对4x4的块，需要把它分成两个4x2或者2x4的块，如果这两个长方形块的颜色相差很大，就还是用原来的ETC编码，如果颜色相差很小，就用改进的ETC编码。对4x4的块，改进的ETC编码的位的组成如下：</p>
<ol>
<li>标识是常规的ETC编码还是基于差值的ETC编码：共1bit</li>
<li>标识两个长方形块是4x2的还是2x4的：共1bit</li>
<li>两个长方形块的基准颜色：R5G5B5 + R3G3B3，共24bit</li>
<li>两个长方形块的亮度编码表索引：共3*2=6bit</li>
<li>每个纹素需要两个bit来从表中某一列取出某个亮度值：共2*16=32bit</li>
</ol>
<p>所以，优化之后基于插值的ETC的编码的存储位数还是$1+1+5<em>3+3</em>3+3<em>2+2</em>16=64bit$。</p>
<h4 id="ETC2"><a href="#ETC2" class="headerlink" title="ETC2"></a><font size="3" color="blue">ETC2</font></h4><p>OpenGL ES3.0集成了ETC2压【25】缩算法。之前的ETC存在的问题是它是根据一个基准色和亮度来拟合纹素的颜色的，在4x4的块里所有纹素都用的同一个基准色，但是如果块里纹素之间原来的颜色实际上相差很大，就会导致ETC拟合出来的结果不好，如下图所示（图中为了简便只画出了B、G两个分量）：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_9.png?raw=true" alt><br>在基于差值的ETC编码中，基准色是5个bit，所以它的范围是[0,31]，差值是3个bit，其范围是[-4,3]，而实际上通过基准色加上差值得到的另一个块的基准色的范围可能不在[0,31]的范围，会存在溢出的情况，比如基准色是1，而差值是-3。所有溢出情况如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_13.png?raw=true" alt><br>溢出的颜色肯定是不符合物理的。在ETC2中，感觉它会认为发生溢出的两个块就是颜色相差很大的两个块（不知道是不是这样认为的，而且也不清楚这个差值到底是怎么算出来的，为什么溢出就会色差很大？）。然后它会对发生溢出的4x4的块采用新的压缩编码方式，并且需要保证编码之后的位的数量不会大于原来的64。下面针对单通道来说，首先，还是需要用一个bit，来标识是否是基于差值的编码是，然后两个4x2的子块的基准色（R5+D3）是需要的。就是这里，实际上对发生溢出的块来说是不需要8bit这么多的。看上面那张表，可以看到基准色R0和差值dR构成的位序列，可以用最右边那列的4个位组成的序列来唯一替换（因为是一一对应的，所以解码的时候能够根据4位的序列对应回原来的8位序列）。所以两个子块的基准色可以压缩到4个bit。相比于原来ETC两个子块需要64个bit，那么还剩下64-1-4=59bit可以用来编码。而怎么用这59bit来编码，RTC2有3种模式：</p>
<ol>
<li>T型模式：<br>如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_10.png?raw=true" alt><br>该模式适用于那些有大部分纹素色度类似，但是有小部分纹素色度偏差很大的情况。该模式下会生成4个颜色，其中2个是原来的两个块的基准颜色，如上图中间图中左上角的圆点和右下角中间的圆点。另外两个颜色是纹素色度聚集的那个基准颜色（右下角中间的圆点）加上$(-d,-d,-d)$和$(d,d,d)$之后生成的。$d$来源于一个算法固定的查找表（look-up date，LUT）：${3,6,11,16,23,32,41,64}$。块里会记录一个3bit的索引，用于从查找表中取出一个$d$值。然后以此为该块生成4个颜色。块中的每个纹素还记录了2个bit，用来从这4个颜色中选择一个作为纹素最终的颜色。对4x4的块总共需要存储$12<em>2+3+16</em>2=59bit$（没太懂为啥还需要2个12bit来存储基准色~）。从上面可以看出，由于它将两个色差较大的长方形块联系在了一起，通过两个颜色来调制出块里每个纹素的颜色，所以压缩效果会比ETC好。</li>
<li>H型模式：<br>如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_11.png?raw=true" alt><br>该模式适用于那些明显呈现两部分不同的色度，且纹素数量差不多的情况。从图中可以看出编码方式和T型模式类似，它会用两个基准色去加$(d,d,d)$和$(-d,-d,-d)$来得到4个颜色（如图中的四个红色圆点所示）。然后纹素再根据自己的2bit去其中选择一个颜色作为最终的颜色。不同的是，在R通道没有溢出而G通道溢出的情况下，需要1bit来确保R通道没有溢出，那么编码这种情况就只剩下$59-1=58bit$，所以H型模式就需要找到某个地方省出1bit来：由于H型模式是对称的，交换两个基准色会得到相同的编码结果，所以可以通过去掉这种冗余来省出这1bit——当基准色0小于基准色1时，让$d$（之前是3bit表示的）中的某一位等于0；当基准色0大于基准色1时，让$d$中的这一位等于1，这样就相当于对$d$中的那一位进行硬编码了，所以再来两个bit就能索引完上面的8个元素的查找表。如此，就能省出来一个bit，来标识R通过是否溢出。</li>
<li>Planar型模式<br>如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_12.png?raw=true" alt><br>该模式适用于那些想要块内的颜色平滑渐变的情况。可以直接在块内存储三个R6G7B6的基准色（如图中的$C_0$、$C_V$和$C_H$）。然后块内纹素的颜色直接通过线性插值得到：$C(x,y)=x(C_H-C_0)/4+y(C_V-C_0)/4+C_0$</li>
</ol>
<p>而EAC（Ericsson alpha compression）【26】是专门用来压缩单通道图像的。压缩原理和ETC类似，压缩结果是平均每个纹素需要存储4个bit。EAC可以和ETC2结合使用，也可以使用两个EAC通道来压缩法线。ETC、ETC2、EAC都被集成到了OpenGL4.0、OpenGL ES3.0、Vulkan以及Metal里。</p>
<h4 id="压缩法线贴图"><a href="#压缩法线贴图" class="headerlink" title="压缩法线贴图"></a><font size="3" color="blue">压缩法线贴图</font></h4><p>通常对RGB颜色的压缩方式并不适用于法线贴图。很多针对法线贴图的压缩方法都利用了法线的这个特点：法线是长度为1的单位向量，并且z值一定是正的（基于法线是切线空间的前提下）。那么z分量就可以通过其他两个分量算出来：$n_z=\sqrt{1-n_x^2-n_y^2}$。所以，对法线我们只需要存储x、y两个分量，这实际上已经是一种压缩了。进一步压缩的话，可以用之前介绍的BC5或者双通道EAC来压缩x和y通道。在不支持BC5或者EAC的硬件上，替补方案是使用BC3，然后把法线的xy分量存储到G通道和A通道里【27】，因为通常这两个通道的精度更高（即占的bit更多）。</p>
<h4 id="PVRTC"><a href="#PVRTC" class="headerlink" title="PVRTC"></a><font size="3" color="blue">PVRTC</font></h4><p>PVRTC【29】是在iPhone和iPad上用得很多的纹理压缩方案。它不是基于块的压缩方案。它是通过两张分辨率较低的低频图像，加上另一张高频的调制信号图像（图像中每个纹素存储1或者2bit），来压缩和恢复原始图像。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_14.png?raw=true" alt><br>先对原始图像用小波变换（wavelet transform）进行滤波，然后原始图像减去滤波的图像得到差值图像（ delta image），然后根据这个差值图像的基本特征向量（ principal eigenvector）或者翻转差值向量（ delta<br>vector）来得到轴向图（axis image），最后根据这个轴向图以及迭代细化过程，得到那两张低频图像。调制信号图像怎么出来的没看懂~。</p>
<h4 id="ASTC"><a href="#ASTC" class="headerlink" title="ASTC"></a><font size="3" color="blue">ASTC</font></h4><p>ASTC（adaptive scalable texture compression）是OpenGL ES 3.2中的另一种压缩纹理格式。它也是基于块的方式，不同的是，它可以支持很多不同大小的块（甚至块可以是长方形的，边长也不需要是2的幂），而且无论块多大，都会用128bit来编码，所以不同的块也就有不同的比特率（bit rate，和之前说的每纹素/像素存储多少个bit是一样的）。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_15.png?raw=true" alt><br>ASTC用到了一种叫BISE（bounded integer sequence encoding）的位压缩思想。比如要编码3, 79, 37这几个数字，由于我们事先知道最大的是79，所以用7个bit来表示它们就可以了：0000011、1001111以及0100101。但是7个bit实际上可以表示的范围可以达到127，比我们要编码的最大的79大多了，这就造成了浪费。有没有可能用更少的bit刚好表示到79左右的范围？我们注意到$79&lt;80=5<em>2^4$，后面的$2^4$可以用4个bit来表示，也就是说我们可以用一个[1,5]的十进制数（对应二进制表示里的高3位）加上4个bit表示的二进制数（对应二进制表示里的低4位），如果用3个bit来表示前面那个[1,5]的十进制数就和原来一样了，3个bit都可以表示到8了，不划算。其实我们现在只需要编码3,79,37这三个数字，这三个数字的高3位都只在[1,5]的范围，总共组合也就只有$5^3=125$种，所以可以用7bit来表示这125种组合情况，而原来三个数字的高3位需要用$3</em>3=9bit$。所以BISE就是这样达到了压缩编码的意图，而且还是无损压缩（只是BISE是无损压缩，ASTC本身是有损压缩）。</p>
<p>和ETC等等基于块的压缩方式一样，ASTC会对每个块存储两个基准颜色（论文里叫endpoints），对块里的某些texel记录其插值系数（论文里也叫Color Weights），之所以是某些，是因为不用对块里的每个texel都记录一个插值系数，那些没记录的可以通过线性插值计算出来其插值系数。如果块内有好几堆差异很大的颜色，ASTC会自动对这些颜色进行分区（partion），对每个分区单独存储其基准颜色，所以这种情况下，纹素会先去找到自己属于哪个分区，再在分区内进行插值得到自己最终的颜色。而块内这些分区ID、基准颜色、插值系数等等信息都是按照BISE方式来进行压缩存储的。</p>
<p>ASTC的优点是：</p>
<ol>
<li>有很高的灵活性</li>
<li>可变的压缩率，可以调整压缩率来权衡压缩速度和质量</li>
<li>支持压缩编码任意通道数量（1到4个之间）的纹理</li>
<li>支持压缩编码HDR、LDR纹理</li>
<li>支持压缩编码3D纹理</li>
<li>压缩效果通常也比BC、ETC、PVRTC等等要好</li>
</ol>
<h4 id="改善压缩贴图质量"><a href="#改善压缩贴图质量" class="headerlink" title="改善压缩贴图质量"></a><font size="3" color="blue">改善压缩贴图质量</font></h4><p>文献【30】中提出了几种可以改善压缩贴图质量的方法：</p>
<ol>
<li>建议颜色纹理和法线纹理每个通道使用16bit。</li>
<li>对颜色纹理，执行一个<a href="https://robotacademy.net.au/lesson/histogram-normalization/" target="_blank" rel="noopener">直方图再归一化</a>（histogram renormalization）（感觉很像<a href="https://zh.wikipedia.org/wiki/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96" target="_blank" rel="noopener">直方图均衡化</a>），然后在Shader里使用scale和offset来调整效果。因为直方图再归一化这种技术可以把图像中的颜色值重新扩展（映射）到整个颜色空间，能够有效地增大图像的对比度。并且每个通道16bit结合上直方图归一化，能够解决掉很多压缩方案会出现的条带伪影（banding artifacts），如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_6_16.png?raw=true" alt></li>
<li>如果纹理中75%的像素值都大于116/255，建议把纹理存储在线性空间（比如法线贴图）；否则的话建议把纹理存储在sRGB空间。</li>
<li>使用BC5/3Dc压缩法线贴图的时候，压缩算法都是把x和y分量分开进行压缩，完全独立，这样可能导致不能很好地恢复出原来的法线方向，该文献中提出了一种衡量法线压缩误差的公式：<br>$$<br>e=arccos(\frac{n\cdot n_c}{||n||~||n_c||})<br>$$<br>其中$n$是原来的法线，$n_c$是压缩后被解压出来的法线。</li>
<li>可以在$YC_oC_g$空间里压缩贴图，即用$YC_oC_g$来代替$RBG$，其中$Y$是亮度，$C_o$和$C_g$是色度。$RGB$转换到$YC_oC_g$的公式如下：<br>$$<br>\begin{bmatrix}<br>Y<br>\end{bmatrix}<br>$$</li>
</ol>
<hr>
<p>参考文献：<br>【1】Geiss, Ryan, “Generating Complex Procedural Terrains Using the GPU,” in Hubert Nguyen, ed., GPU Gems 3, Addison-Wesley, pp. 7–37, 2007. Cited on p. 171<br>【2】 Geiss, Ryan, and Michael Thompson, “NVIDIA Demo Team Secrets—Cascades,” Game Developers Conference, Mar. 2007. Cited on p. 171, 571<br>【3】 Tarini, Marco, Kai Hormann, Paolo Cignoni, and Claudio Montani, “PolyCube-Maps,” ACM Transactions on Graphics (SIGGRAPH 2004), vol. 23, no. 3, pp. 853–860, Aug. 2004. Cited on p. 171<br>【4】 Hormann, Kai, Bruno L´ evy, and Alla Sheffer, SIGGRAPH Mesh Parameterization: Theory and Practice course, Aug. 2007. Cited on p. 173<br>【5】 Ruijters, Daniel, Bart M. ter Haar Romeny, and Paul Suetens, “Efficient GPU-Based Texture Interpolation Using Uniform B-Splines,” Journal of Graphics, GPU, and Game Tools, vol. 13, no. 4, pp. 61–69, 2008. Cited on p. 180, 733, 734<br>【6】  Crow, Franklin C., “Summed-Area Tables for Texture Mapping,” Computer Graphics (SIGGRAPH ’84 Proceedings), vol. 18, no. 3, pp. 207–212, July 1984. Cited on p. 186<br>【7】Heckbert, Paul S., “Fundamentals of Texture Mapping and Image Warping,” Technical Report 516, Computer Science Division, University of California, Berkeley, June 1989. Cited on p. 187, 189, 222, 688<br>【8】Green, Simon, “Summed Area Tables Using Graphics Hardware,” Game Developers Conference, Mar. 2003. Cited on p. 188<br>【9】Hensley, Justin, and Thorsten Scheuermann, “Dynamic Glossy Environment Reflections Using Summed-Area Tables,” in Wolfgang Engel, ed., ShaderX 4 , Charles River Media, pp. 187–200, 2005. Cited on p. 188, 419<br>【10】 Hensley, Justin, Thorsten Scheuermann, Greg Coombe, Montek Singh, and Anselmo Lastra,“Fast Summed-Area Table Generation and Its Applications,” Computer Graphics Forum, vol. 24, no. 3, pp. 547–555, 2005. Cited on p. 188, 419<br>【11】 Lauritzen, Andrew, “Summed-Area Variance Shadow Maps,” in Hubert Nguyen, ed., GPU Gems 3, Addison-Wesley, pp. 157–182, 2007. Cited on p. 188, 252, 253, 255<br>【12】Hensley, Justin, and Thorsten Scheuermann, “Dynamic Glossy Environment Reflections Using Summed-Area Tables,” in Wolfgang Engel, ed., ShaderX 4 , Charles River Media, pp. 187–200, 2005. Cited on p. 188, 419<br>【13】Heckbert, Paul S., “Fundamentals of Texture Mapping and Image Warping,” Technical Report 516, Computer Science Division, University of California, Berkeley, June 1989. Cited on p. 187, 189, 222, 688<br>【14】 Mavridis, Pavlos, and Georgios Papaioannou, “High Quality Elliptical Texture Filtering on GPU,” in Symposium on Interactive 3D Graphics and Games, ACM, pp. 23–30, Feb. 2011. Cited on p. 189<br>【15】Benson, David, and Joel Davis, “Octree Textures,” ACM Transactions on Graphics (SIGGRAPH 2002), vol. 21, no. 3, pp. 785–790, July 2002. Cited on p. 190<br>【16】 DeBry, David (grue), Jonathan Gibbs, Devorah DeLeon Petty, and Nate Robins, “Painting and Rendering Textures on Unparameterized Models,” ACM Transactions on Graphics (SIGGRAPH 2002), vol. 21, no. 3, pp. 763–768, July 2002. Cited on p. 190<br>【17】 Lefebvre, Sylvain, Samuel Hornus, and Fabrice Neyret, “Octree Textures on the GPU,” in Matt Pharr, ed., GPU Gems 2, Addison-Wesley, pp. 595–613, 2005. Cited on p. 190<br>【18】Lefebvre, Sylvain, and Hugues Hoppe, “Perfect Spatial Hashing,” ACM Transactions on Graphics, vol. 25, no. 3, pp. 579–588, July 2006. Cited on p. 190<br>【19】NVIDIA Corporation, “Improve Batching Using Texture Atlases,” SDK White Paper, 2004. Cited on p. 191<br>【20】 Manson, Josiah, and Scott Schaefer, “Parameterization-Aware MIP-Mapping,” Computer Graphics Forum, vol. 31, no. 4, pp. 1455–1463, 2012. Cited on p. 191<br>【21】 Burley, Brent, and Dylan Lacewell, “Ptex: Per-Face Texture Mapping for Production Rendering,” in Proceedings of the Nineteenth Eurographics Conference on Rendering, Eurographics Association, pp. 1155–1164, 2008. Cited on p. 191<br>【22】Everitt, Cass, Graham Sellers, John McDonald, and Tim Foley, “Approaching Zero Driver Overhead,” Game Developers Conference, Mar. 2014. Cited on p. 191, 192<br>【23】“S3TC DirectX 6.0 Standard Texture Compression,” S3 Inc. website, 1998. Cited on p. 192<br>【24】 Ström, Jacob, and Tomas Akenine-Möller, “iPACKMAN: High-Quality, Low-Complexity Texture Compression for Mobile Phones,” in Graphics Hardware 2006, Eurographics Association, pp. 63–70, July 2005. Cited on p. 194<br>【25】Ström, Jacob, and Martin Pettersson, “ETC2: Texture Compression Using Invalid Combinations,” in Graphics Hardware 2007, Eurographics Association, pp. 49–54, Aug. 2007. Cited on p. 194<br>【26】Wennersten, Per, and Jacob Ström, “Table-Based Alpha Compression,” Computer Graphics Forum, vol. 28, no. 2, pp. 687–695, 2009. Cited on p. 194<br>【27】Mittring, Martin, “Finding Next Gen—CryEngine 2,” SIGGRAPH Advanced Real-Time Rendering in 3D Graphics and Games course, Aug. 2007. Cited on p. 43, 195, 239, 242, 255, 457, 476, 559, 856, 860, 861<br>【28】Fenney, Simon, “Texture Compression Using Low-Frequency Signal Modulation,” in Graphics Hardware 2003, Eurographics Association, pp. 84–91, July 2003. Cited on p. 196<br>【29】Fenney, Simon, “Texture Compression Using Low-Frequency Signal Modulation,” in Graphics Hardware 2003, Eurographics Association, pp. 84–91, July 2003. Cited on p. 196</p>
<h1 id="15-Non-Photorealistic-Rendering"><a href="#15-Non-Photorealistic-Rendering" class="headerlink" title="15. Non-Photorealistic Rendering"></a><font size="6" color="orange">15. Non-Photorealistic Rendering</font></h1><hr>
<h2 id="Outline-Rendering"><a href="#Outline-Rendering" class="headerlink" title="Outline Rendering"></a><font size="5" color="red">Outline Rendering</font></h2><h3 id="法线和视线夹角"><a href="#法线和视线夹角" class="headerlink" title="法线和视线夹角"></a><font size="4" color="green">法线和视线夹角</font></h3><p>第一种方法是根据法线和视线的夹角来渲染轮廓。当法线和视线的夹角接近于0时，就很可能是轮廓线（contour edge）。如下图所示：<br><img src="https://github.com/Popperelay/Photos/blob/master/Photos/RealtimeRendering_15_4.png?raw=true" alt><br>这个方法的特点也是它的缺点：轮廓线的宽度取决于表面的曲率。换句话说，它不适用于像立方体这种有折痕的模型，而更适用于曲面模型，需要在轮廓线的附近，真正存在法线和视线接近垂直的像素，不然渲染不出来轮廓。而且如果模型离视点较远，轮廓线上的法线可能不会接近于垂直视线，导致无法画出正常的轮廓线。参考文献[5]中提出了结合光照、曲率和距离来确定轮廓线宽度的方法。</p>
<p>参考文献：<br>[5] Goodwin, Todd, Ian Vollick, and Aaron Hertzmann, “Isophote Distance: A Shading Approach to Artistic Stroke Thickness,” Proceedings of the 5th International Symposium on Non-Photorealistic Animation and Rendering, ACM, pp. 53–62, Aug. 2007. Cited on p. 657, 667</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2019/10/14/RealtimeRendering/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://yoursite.com/2019/10/14/RealtimeRendering/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
            	<span id="busuanzi_container_site_pv">2019总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
